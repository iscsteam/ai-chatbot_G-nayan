{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iscsteam/ai-chatbot_G-nayan/blob/main/rag_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwnNlNIEwoZ8"
      },
      "source": [
        "To learn more about accelerating pandas on Colab, see the [10 minute guide](https://colab.research.google.com/github/rapidsai-community/showcase/blob/main/getting_started_tutorials/cudf_pandas_colab_demo.ipynb) or\n",
        " [US stock market data analysis demo](https://colab.research.google.com/github/rapidsai-community/showcase/blob/main/getting_started_tutorials/cudf_pandas_stocks_demo.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lh3c_hSjBIaY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96d7fbcc-533e-4d5b-875c-95cf4992ffd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.1/611.1 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.3/79.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.2/122.2 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m107.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "! pip install langchain -q\n",
        "! pip install langchain_community -q\n",
        "! pip install langchain_groq -q\n",
        "! pip install grandalf -q\n",
        "! pip install numpy -q\n",
        "! pip install pandas -q\n",
        "! pip install sentence-transformers -q\n",
        "! pip install faiss-cpu -q\n",
        "! pip install groq -q\n",
        "! pip install PyPDF2 -q\n",
        "! pip install rank_bm25 langchain pinecone-client -q\n",
        "! pip install pymupdf -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "02FIxuk69CAB",
        "outputId": "bcd1fe61-6c47-464f-fbdd-7b2c70e9a655"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "cudf-cu12 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade langchain langchain_chroma langchain_community \\\n",
        "    langchain_groq grandalf numpy pandas sentence-transformers \\\n",
        "    faiss-cpu langchain-deepseek-official groq PyPDF2 -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQl1OLu_25B2",
        "outputId": "1761443c-c5da-40a5-8d9f-393b164e8bbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rMiqSR3J2G1W"
      },
      "outputs": [],
      "source": [
        "import langchain\n",
        "import os\n",
        "from groq import Groq\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "from langchain_community.embeddings import OllamaEmbeddings\n",
        "from langchain_community import embeddings\n",
        "from langchain.schema import Document\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.schema import Document\n",
        "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "import glob\n",
        "from rank_bm25 import BM25Okapi\n",
        "import re\n",
        "import getpass\n",
        "from dotenv import load_dotenv\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "from langchain_community.embeddings import OllamaEmbeddings\n",
        "from langchain_community import embeddings\n",
        "from langchain.schema import Document\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema import SystemMessage, HumanMessage\n",
        "from langchain.agents import initialize_agent, AgentType\n",
        "from langchain.tools import Tool\n",
        "from langchain.memory import ConversationBufferMemory"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain.agents import initialize_agent, AgentType"
      ],
      "metadata": {
        "id": "bcrnj0w3iDm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "print(f\"Python Version: {sys.version}\")\n",
        "# Or, for a cleaner version string:\n",
        "!python --version\n",
        "import groq\n",
        "print(groq.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTnryrK59H40",
        "outputId": "8a1575c0-2cd4-43fa-cf44-effa41c89137"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python Version: 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\n",
            "Python 3.11.11\n",
            "0.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain\n",
        "import groq\n",
        "import rank_bm25\n",
        "import PyPDF2\n",
        "import dotenv\n",
        "import langchain_community\n",
        "import langchain_groq\n",
        "import sys\n",
        "print(f\"Python Version: {sys.version}\")\n",
        "print(\"LangChain version:\", langchain.__version__)\n",
        "print(\"Groq version:\", groq.__version__)\n",
        "\n",
        "print(\"PyPDF2 version:\", PyPDF2.__version__)\n",
        "\n",
        "print(\"LangChain Community version:\", langchain_community.__version__)\n",
        "print(\"LangChain Groq version:\", langchain_groq.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjiaCR589mcm",
        "outputId": "3ee6ef9d-6f35-4cd1-fcb2-d3fad2c2b04e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python Version: 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\n",
            "LangChain version: 0.3.20\n",
            "Groq version: 0.19.0\n",
            "PyPDF2 version: 3.0.1\n",
            "LangChain Community version: 0.3.19\n",
            "LangChain Groq version: 0.2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "AaS7qENL3XzC",
        "outputId": "f2f8af79-6883-4842-ca1b-790b7a8f402c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'dotenv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-32f25129b3f8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdotenv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dotenv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0menv_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/env_varaible/.env'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# # Load the .env file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dotenv'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "env_file_path = '/content/drive/MyDrive/env_varaible/.env'\n",
        "\n",
        "# # Load the .env file\n",
        "# load_dotenv(env_file_path)\n",
        "# groq_api_key=API_KEY = os.environ[\"groq_api_key\"]\n",
        "# groq_api_key\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9y4pINbuyGSU"
      },
      "outputs": [],
      "source": [
        "pdf_path=r\"/content/drive/MyDrive/Resumes\"\n",
        "pdf_files = glob.glob(os.path.join(pdf_path, \"*.pdf\"))\n",
        "for pdf_file in pdf_files:\n",
        "  def extract_text_from_pdf(pdf_file):\n",
        "        reader = PdfReader(pdf_file)\n",
        "        text = \"\"\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()\n",
        "        return text\n",
        "  text = extract_text_from_pdf(pdf_path)\n",
        "  def clean_whitespace(text):\n",
        "      \"\"\"Removes extra whitespace, tabs, and newlines.\"\"\"\n",
        "      text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces, tabs, newlines with a single space\n",
        "      text = text.strip()  # Remove leading/trailing whitespace\n",
        "      return text\n",
        "\n",
        "  cleaned_text = clean_whitespace(text)\n",
        "\n",
        "  def handle_bullet_points(text):\n",
        "      \"\"\"Replaces Unicode bullet points with standard bullets.\"\"\"\n",
        "      text = text.replace('\\uf0b7', '• ')  # Replace with a standard bullet and a space\n",
        "      return text\n",
        "  cleaned_text = handle_bullet_points(cleaned_text)\n",
        "  # Initialize the Groq client\n",
        "  client = Groq(api_key=groq_api_key)  # Use os.getenv to fetch the environment variable\n",
        "  # Define the function description\n",
        "  resume_function_description = {\n",
        "      \"name\": \"extract_resume_details\",\n",
        "      \"description\": \"Extract metadata and sections from a resume\",\n",
        "      \"parameters\": {\n",
        "          \"type\": \"object\",\n",
        "          \"properties\": {\n",
        "              \"name\": {\"type\": \"string\", \"description\": \"Full name of the candidate\"},\n",
        "              \"email\": {\"type\": \"string\", \"description\": \"Email address\"},\n",
        "              \"phone_number\": {\"type\": \"string\", \"description\": \"Phone number\"},\n",
        "              \"summary\": {\"type\": \"string\", \"description\": \"Summary section of the resume complete summary field dont miss anything\"},\n",
        "              \"skills\": {\"type\": \"string\", \"description\": \"Skills section of the resume technical skills\"},\n",
        "              \"projects\": {\"type\": \"string\", \"description\": \"Projects section full session description bring everything\"},\n",
        "              \"experience\": {\"type\": \"string\", \"description\": \"Work experience section of the resume\"},\n",
        "              \"education\": {\"type\": \"string\", \"description\": \"Education section of the resume\"},\n",
        "          },\n",
        "          \"required\": [\"name\", \"email\", \"phone_number\"],\n",
        "      },\n",
        "  }\n",
        "\n",
        "  # Define the PromptTemplate\n",
        "  system_prompt_template = PromptTemplate(\n",
        "      input_variables=[\"resume_text\"],  # Placeholder for dynamic input\n",
        "      template=\"\"\"\n",
        "  You are an AI Assistant. You will be given a resume text and need to convert it into a structured JSON format as described by the function `extract_resume_details`.\n",
        "  If the data for a field is not provided in the resume, return \"not provided\" for that field.\n",
        "  Strictly adhere to the format and properties defined in `extract_resume_details`\n",
        "  for the \"summary\" field extract the complete summary section,\n",
        "  For the \"projects\" field, extract **all details** including:\n",
        "  - The project title.\n",
        "  - A **detailed** description of the project.\n",
        "  - Tools and technologies used.\n",
        "  - Methodologies implemented.\n",
        "  - Outcomes or results achieved.\n",
        "  - dont forget any information and extract all the details\n",
        "  Do not omit any details provided in the resume about the projects Ensure all available information is captured completely.**\n",
        "\n",
        "  Resume Text:\n",
        "  {resume_text}\n",
        "      \"\"\"\n",
        "  )\n",
        "\n",
        "\n",
        "  # Generate the formatted system prompt\n",
        "  system_prompt = system_prompt_template.format(resume_text=cleaned_text)\n",
        "\n",
        "  # Call the model with the formatted system prompt\n",
        "  chat_completion = client.chat.completions.create(\n",
        "      model=\"llama-3.3-70b-versatile\",\n",
        "      messages=[\n",
        "          {\n",
        "              \"role\": \"system\",\n",
        "              \"content\": system_prompt\n",
        "          },\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": cleaned_text\n",
        "          }\n",
        "      ],\n",
        "      functions=[resume_function_description],  # Pass the function description here\n",
        "      function_call={\"name\": \"extract_resume_details\"},  # Tell the model to use this function\n",
        "      max_tokens=2000,\n",
        "      temperature=0.4,\n",
        "  )\n",
        "  # Parse the JSON response from the model\n",
        "  try:\n",
        "      function_args = json.loads(chat_completion.choices[0].message.function_call.arguments)\n",
        "      #print(json.dumps(function_args, indent=4))  # Print nicely formatted JSON\n",
        "  except json.JSONDecodeError as e:\n",
        "      print(f\"Error decoding JSON: {e}\")\n",
        "      print(\"Raw response content:\", chat_completion.choices[0].message.content)  # Print raw content if JSON parsing fails\n",
        "  except AttributeError as e:\n",
        "      print(f\"Function call arguments not found in response: {e}\")\n",
        "      print(\"Raw response content:\", chat_completion.choices[0].message.content)  #\n",
        "\n",
        "  meta_data = {\n",
        "      \"name\": function_args.get(\"name\"),\n",
        "      \"email\": function_args.get(\"email\"),\n",
        "      \"phone_number\": function_args.get(\"phone_number\"),\n",
        "  }\n",
        "  doc=function_args\n",
        "  content = f\"\"\"\n",
        "  name: {function_args.get('name')}\n",
        "  phone: {function_args.get('phone')}\n",
        "  email: {function_args.get('email')}\n",
        "  summary: {function_args.get('summary')}\n",
        "  education: {function_args.get('education')}\n",
        "  experience: {function_args.get('experience')}\n",
        "  skills: {function_args.get('skills')}\n",
        "  projects: {function_args.get('projects')}\n",
        "  \"\"\"\n",
        "  docs=Document(page_content=content,metadata=meta_data)\n",
        "documents=[doc]\n",
        "# Initialize the HuggingFace Embedding model\n",
        "embed_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "vectore_store_FAISS=FAISS.from_documents([documents],embedding=embed_model)\n",
        "vectore_store_FAISS.save_local(\"faiss_index\")\n",
        "# Save FAISS index and metadata to Google Drive\n",
        "save_path = \"/content/drive/MyDrive/fasis_db/resume_index_faiss\"\n",
        "vectore_store_FAISS.save_local(save_path)\n",
        "print(f\"FAISS index saved to {save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGLm4lbjy7MX"
      },
      "outputs": [],
      "source": [
        "docs=Document(page_content=content,metadata=meta_data)\n",
        "documents=[doc]\n",
        "# Initialize the HuggingFace Embedding model\n",
        "embed_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "vectore_store_FAISS=FAISS.from_documents([documents],embedding=embed_model)\n",
        "vectore_store_FAISS.save_local(\"faiss_index\")\n",
        "# Save FAISS index and metadata to Google Drive\n",
        "save_path = \"/content/drive/MyDrive/fasis_db/resume_index_faiss\"\n",
        "vectore_store_FAISS.save_local(save_path)\n",
        "print(f\"FAISS index saved to {save_path}\")\n",
        "# Initialize the HuggingFace Embedding model\n",
        "embed_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "\n",
        "# # Embed the content (as a list of documents)\n",
        "# vectors = embed_model.embed_documents([content])\n",
        "\n",
        "# # Print the resulting vector\n",
        "# print(\"Generated Vector:\")\n",
        "# print(vectors[0])  # S\n",
        "#embed_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "vectore_store_FAISS=FAISS.from_documents([documents],embedding=embed_model)\n",
        "vectore_store_FAISS.save_local(\"faiss_index\")\n",
        "# Save FAISS index and metadata to Google Drive\n",
        "save_path = \"/content/drive/MyDrive/fasis_db/resume_index_faiss\"\n",
        "vectore_store_FAISS.save_local(save_path)\n",
        "\n",
        "print(f\"FAISS index saved to {save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUatZWA5AkQ_"
      },
      "outputs": [],
      "source": [
        "docs=Document(page_content=content,metadata=meta_data)\n",
        "documents=[doc]\n",
        "# Initialize the HuggingFace Embedding model\n",
        "embed_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "vectore_store_FAISS=FAISS.from_documents([documents],embedding=embed_model)\n",
        "vectore_store_FAISS.save_local(\"faiss_index\")\n",
        "# Save FAISS index and metadata to Google Drive\n",
        "save_path = \"/content/drive/MyDrive/fasis_db/resume_index_faiss\"\n",
        "vectore_store_FAISS.save_local(save_path)\n",
        "print(f\"FAISS index saved to {save_path}\")\n",
        "# Step 1: Extract text from the resume\n",
        "from PyPDF2 import PdfReader\n",
        "import re\n",
        "pdf_path=r\"/content/drive/MyDrive/Resumes/Salvapathi Naidu_Gen-AI.pdf\"\n",
        "\n",
        "\n",
        "def extract_text_from_pdf(pdf_file):\n",
        "    reader = PdfReader(pdf_file)\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        text += page.extract_text()\n",
        "    return text\n",
        "text = extract_text_from_pdf(pdf_path)\n",
        "def clean_whitespace(text):\n",
        "    \"\"\"Removes extra whitespace, tabs, and newlines.\"\"\"\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces, tabs, newlines with a single space\n",
        "    text = text.strip()  # Remove leading/trailing whitespace\n",
        "    return text\n",
        "\n",
        "cleaned_text = clean_whitespace(text)\n",
        "\n",
        "def handle_bullet_points(text):\n",
        "    \"\"\"Replaces Unicode bullet points with standard bullets.\"\"\"\n",
        "    text = text.replace('\\uf0b7', '• ')  # Replace with a standard bullet and a space\n",
        "    return text\n",
        "\n",
        "cleaned_text = handle_bullet_points(cleaned_text)\n",
        "\n",
        "\n",
        "\n",
        "# Initialize the Groq client\n",
        "client = Groq(api_key=groq_api_key)  # Use os.getenv to fetch the environment variable\n",
        "\n",
        "# Define the function description\n",
        "resume_function_description = {\n",
        "    \"name\": \"extract_resume_details\",\n",
        "    \"description\": \"Extract metadata and sections from a resume\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"name\": {\"type\": \"string\", \"description\": \"Full name of the candidate\"},\n",
        "            \"email\": {\"type\": \"string\", \"description\": \"Email address\"},\n",
        "            \"phone_number\": {\"type\": \"string\", \"description\": \"Phone number\"},\n",
        "            \"summary\": {\"type\": \"string\", \"description\": \"Summary section of the resume complete summary field dont miss anything\"},\n",
        "            \"skills\": {\"type\": \"string\", \"description\": \"Skills section of the resume technical skills\"},\n",
        "            \"projects\": {\"type\": \"string\", \"description\": \"Projects section full session description bring everything\"},\n",
        "            \"experience\": {\"type\": \"string\", \"description\": \"Work experience section of the resume\"},\n",
        "            \"education\": {\"type\": \"string\", \"description\": \"Education section of the resume\"},\n",
        "        },\n",
        "        \"required\": [\"name\", \"email\", \"phone_number\"],\n",
        "    },\n",
        "}\n",
        "\n",
        "# Define the PromptTemplate\n",
        "system_prompt_template = PromptTemplate(\n",
        "    input_variables=[\"resume_text\"],  # Placeholder for dynamic input\n",
        "    template=\"\"\"\n",
        "You are an AI Assistant. You will be given a resume text and need to convert it into a structured JSON format as described by the function `extract_resume_details`.\n",
        "If the data for a field is not provided in the resume, return \"not provided\" for that field.\n",
        "Strictly adhere to the format and properties defined in `extract_resume_details`\n",
        "for the \"summary\" field extract the complete summary section,\n",
        "For the \"projects\" field, extract **all details** including:\n",
        "- The project title.\n",
        "- A **detailed** description of the project.\n",
        "- Tools and technologies used.\n",
        "- Methodologies implemented.\n",
        "- Outcomes or results achieved.\n",
        "- dont forget any information and extract all the details\n",
        "Do not omit any details provided in the resume about the projects Ensure all available information is captured completely.**\n",
        "\n",
        "Resume Text:\n",
        "{resume_text}\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "\n",
        "# Generate the formatted system prompt\n",
        "system_prompt = system_prompt_template.format(resume_text=cleaned_text)\n",
        "\n",
        "# Call the model with the formatted system prompt\n",
        "chat_completion = client.chat.completions.create(\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": system_prompt\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": cleaned_text\n",
        "        }\n",
        "    ],\n",
        "    functions=[resume_function_description],  # Pass the function description here\n",
        "    function_call={\"name\": \"extract_resume_details\"},  # Tell the model to use this function\n",
        "    max_tokens=2000,\n",
        "    temperature=0.5,\n",
        ")\n",
        "# Parse the JSON response from the model\n",
        "try:\n",
        "    function_args = json.loads(chat_completion.choices[0].message.function_call.arguments)\n",
        "    #print(json.dumps(function_args, indent=4))  # Print nicely formatted JSON\n",
        "except json.JSONDecodeError as e:\n",
        "    print(f\"Error decoding JSON: {e}\")\n",
        "    print(\"Raw response content:\", chat_completion.choices[0].message.content)  # Print raw content if JSON parsing fails\n",
        "except AttributeError as e:\n",
        "    print(f\"Function call arguments not found in response: {e}\")\n",
        "    print(\"Raw response content:\", chat_completion.choices[0].message.content)  #\n",
        "\n",
        "meta_data = {\n",
        "    \"name\": function_args.get(\"name\"),\n",
        "    \"email\": function_args.get(\"email\"),\n",
        "    \"phone_number\": function_args.get(\"phone_number\"),\n",
        "}\n",
        "doc=function_args\n",
        "content = f\"\"\"\n",
        "name: {function_args.get('name')}\n",
        "phone: {function_args.get('phone')}\n",
        "email: {function_args.get('email')}\n",
        "summary: {function_args.get('summary')}\n",
        "education: {function_args.get('education')}\n",
        "experience: {function_args.get('experience')}\n",
        "skills: {function_args.get('skills')}\n",
        "projects: {function_args.get('projects')}\n",
        "\n",
        "\"\"\"\n",
        "docs1=Document(page_content=content,metadata=meta_data)\n",
        "docs1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "KWgZbwrh1bV3",
        "outputId": "d5b29572-ecd7-424a-ed4b-4a6c12beda41"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'PineconeEmbeddings' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-4babc4fbd871>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    135\u001b[0m   \u001b[0mindex_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPINECONE_INDEX_NAME\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m   \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPineconeEmbeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"multilingual-e5-large\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m   docs_to_vectorstore = PineconeVectorStore.from_documents(\n\u001b[1;32m    139\u001b[0m           \u001b[0mdocuments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moriginal_doc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'PineconeEmbeddings' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "# Step 1: Extract text from the resume\n",
        "from PyPDF2 import PdfReader\n",
        "import re\n",
        "pdf_path=r\"/content/drive/MyDrive/Resumes\"\n",
        "pdf_files = glob.glob(os.path.join(pdf_path, \"*.pdf\"))\n",
        "for pdf_file in pdf_files:\n",
        "  def extract_text_from_pdf(pdf_file):\n",
        "      reader = PdfReader(pdf_file)\n",
        "      text = \"\"\n",
        "      for page in reader.pages:\n",
        "          text += page.extract_text()\n",
        "      return text\n",
        "  text = extract_text_from_pdf(pdf_file)\n",
        "  def clean_whitespace(text):\n",
        "      \"\"\"Removes extra whitespace, tabs, and newlines.\"\"\"\n",
        "      text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces, tabs, newlines with a single space\n",
        "      text = text.strip()  # Remove leading/trailing whitespace\n",
        "      return text\n",
        "\n",
        "  cleaned_text_1 = clean_whitespace(text)\n",
        "\n",
        "  def handle_bullet_points(text):\n",
        "      \"\"\"Replaces Unicode bullet points with standard bullets.\"\"\"\n",
        "      text = text.replace('\\uf0b7', '• ')  # Replace with a standard bullet and a space\n",
        "      return text\n",
        "\n",
        "  cleaned_text = handle_bullet_points(cleaned_text_1)\n",
        "\n",
        "\n",
        "\n",
        "  # Initialize the Groq client\n",
        "  client = Groq(api_key=groq_api_key)  # Use os.getenv to fetch the environment variable\n",
        "\n",
        "  # Define the function description\n",
        "  resume_function_description = {\n",
        "      \"name\": \"extract_resume_details\",\n",
        "      \"description\": \"Extract metadata and sections from a resume\",\n",
        "      \"parameters\": {\n",
        "          \"type\": \"object\",\n",
        "          \"properties\": {\n",
        "              \"name\": {\"type\": \"string\", \"description\": \"Full name of the candidate\"},\n",
        "              \"email\": {\"type\": \"string\", \"description\": \"Email address\"},\n",
        "              \"phone_number\": {\"type\": \"string\", \"description\": \"Phone number\"},\n",
        "              \"summary\": {\"type\": \"string\", \"description\": \"Summary section of the resume complete summary field dont miss anything\"},\n",
        "              \"skills\": {\"type\": \"string\", \"description\": \"Skills section of the resume technical skills\"},\n",
        "              \"projects\": {\"type\": \"string\", \"description\": \"Projects section full session description bring everything\"},\n",
        "              \"experience\": {\"type\": \"string\", \"description\": \"Work experience section of the resume\"},\n",
        "              \"education\": {\"type\": \"string\", \"description\": \"Education section of the resume\"},\n",
        "          },\n",
        "          \"required\": [\"name\", \"email\", \"phone_number\"],\n",
        "      },\n",
        "  }\n",
        "\n",
        "  # Define the PromptTemplate\n",
        "  system_prompt_template = PromptTemplate(\n",
        "      input_variables=[\"resume_text\"],  # Placeholder for dynamic input\n",
        "      template=\"\"\"\n",
        "  You are an AI Assistant. You will be given a resume text and need to convert it into a structured JSON format as described by the function `extract_resume_details`.\n",
        "  If the data for a field is not provided in the resume, return \"not provided\" for that field.\n",
        "  Strictly adhere to the format and properties defined in `extract_resume_details`\n",
        "  for the \"summary\" field extract the complete summary section,\n",
        "  For the \"projects\" field, extract **all details** including:\n",
        "  - The project title.\n",
        "  - A **detailed** description of the project.\n",
        "  - Tools and technologies used.\n",
        "  - Methodologies implemented.\n",
        "  - Outcomes or results achieved.\n",
        "  - dont forget any information and extract all the details\n",
        "  Do not omit any details provided in the resume about the projects Ensure all available information is captured completely.**\n",
        "\n",
        "  Resume Text:\n",
        "  {resume_text}\n",
        "      \"\"\"\n",
        "  )\n",
        "\n",
        "\n",
        "  # Generate the formatted system prompt\n",
        "  system_prompt = system_prompt_template.format(resume_text=cleaned_text)\n",
        "\n",
        "  # Call the model with the formatted system prompt\n",
        "  chat_completion = client.chat.completions.create(\n",
        "      model=\"llama-3.3-70b-versatile\",\n",
        "      messages=[\n",
        "          {\n",
        "              \"role\": \"system\",\n",
        "              \"content\": system_prompt\n",
        "          },\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": cleaned_text\n",
        "          }\n",
        "      ],\n",
        "      functions=[resume_function_description],  # Pass the function description here\n",
        "      function_call={\"name\": \"extract_resume_details\"},  # Tell the model to use this function\n",
        "      max_tokens=20000,\n",
        "      temperature=0.5,\n",
        "  )\n",
        "  # Parse the JSON response from the model\n",
        "  try:\n",
        "      function_args = json.loads(chat_completion.choices[0].message.function_call.arguments)\n",
        "      #print(json.dumps(function_args, indent=4))  # Print nicely formatted JSON\n",
        "  except json.JSONDecodeError as e:\n",
        "      print(f\"Error decoding JSON: {e}\")\n",
        "      print(\"Raw response content:\", chat_completion.choices[0].message.content)  # Print raw content if JSON parsing fails\n",
        "  except AttributeError as e:\n",
        "      print(f\"Function call arguments not found in response: {e}\")\n",
        "      print(\"Raw response content:\", chat_completion.choices[0].message.content)  #\n",
        "\n",
        "  meta_data = {\n",
        "      \"name\": function_args.get(\"name\"),\n",
        "      \"email\": function_args.get(\"email\"),\n",
        "      \"phone_number\": function_args.get(\"phone_number\"),\n",
        "  }\n",
        "  doc=function_args\n",
        "  content = f\"\"\"\n",
        "  name: {function_args.get('name')}\n",
        "  phone: {function_args.get('phone')}\n",
        "  email: {function_args.get('email')}\n",
        "  summary: {function_args.get('summary')}\n",
        "  skills: {function_args.get('skills')}\n",
        "  education: {function_args.get('education')}\n",
        "  experience: {function_args.get('experience')}\n",
        "  projects: {function_args.get('projects')}\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  doc=Document(page_content=content,metadata=meta_data)\n",
        "  PINECONE_API_KEY = os.environ[\"PINECONE_API_KEY\"]\n",
        "  PINECONE_ENVIRONMENT = os.environ[\"PINECONE_ENVIRONMENT\"]\n",
        "  PINECONE_INDEX_NAME = os.environ[\"PINECONE_INDEX_NAME\"]\n",
        "  original_doc=doc\n",
        "  original_doc=[original_doc]\n",
        "  pinecone = PineconeClient(api_key=PINECONE_API_KEY,\n",
        "                          environment=PINECONE_ENVIRONMENT)\n",
        "  index_name = PINECONE_INDEX_NAME\n",
        "\n",
        "  embeddings = PineconeEmbeddings(model=\"multilingual-e5-large\")\n",
        "  docs_to_vectorstore = PineconeVectorStore.from_documents(\n",
        "          documents=original_doc,\n",
        "          index_name=\"resume-retrival\",\n",
        "          embedding=embeddings\n",
        "      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyGta_2wjJmH"
      },
      "outputs": [],
      "source": [
        "query = \"Front-end & back-end development, Product Engineering, Agile software development, Test-driven development, Team Management & Leadership, Azure, AWS, Docker, Kubernetes, Jenkins, MongoDB, PostgreSQL, RabbitMQ, Python, Django, Flask, FastAPI, AI, Data Science, Spark, Kafka, Git, gRPC\"\n",
        "# Perform the vector search\n",
        "vector_search_results = docs_to_vectorstore.similarity_search(query,k=10)\n",
        "\n",
        "for idx, doc in enumerate(vector_search_results):\n",
        "    print(f\"Result {idx + 1}:\")\n",
        "    print(f\"Metadata: {doc.metadata}\")  # Extract metadata\n",
        "    print(f\"Content: {doc.page_content[:1000000]}\")  # Display first 500 characters of the content\n",
        "    print(\"-\" * 90)  # Separator for better readability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lSw0MmBOjne"
      },
      "outputs": [],
      "source": [
        "def hybrid_search(query, top_k=10):\n",
        "    query_embedding = get_embedding(query)\n",
        "\n",
        "    # Dense Search (Pinecone)\n",
        "    pinecone_index = pinecone.Index(\"your-index\")\n",
        "    dense_results = pinecone_index.query(vector=query_embedding, top_k=top_k, include_metadata=True)\n",
        "\n",
        "    # Sparse Search (BM25)\n",
        "    bm25_scores = bm25.get_scores(query.split())\n",
        "    sparse_results = sorted(enumerate(bm25_scores), key=lambda x: x[1], reverse=True)[:top_k]\n",
        "\n",
        "    # Fusion & Re-ranking\n",
        "    final_results = rerank(dense_results, sparse_results)\n",
        "\n",
        "    return final_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wx0c34hhA_MP"
      },
      "outputs": [],
      "source": [
        "embed_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "embed_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TN-T5Dn8E1Qn"
      },
      "outputs": [],
      "source": [
        "\n",
        "content = f\"\"\"\n",
        "name: {resumr_jason_formate.get('name')}\n",
        "phone: {resumr_jason_formate.get('phone')}\n",
        "email: {resumr_jason_formate.get('email')}\n",
        "summary: {resumr_jason_formate.get('summary')}\n",
        "education: {resumr_jason_formate.get('education')}\n",
        "experience: {resumr_jason_formate.get('experience')}\n",
        "projects: {resumr_jason_formate.get('projects')}\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "doc=Document(page_content=content,metadata=meta_data)\n",
        "doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8F7ojySV2op"
      },
      "outputs": [],
      "source": [
        "docs=list([doc])\n",
        "docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlYDxNr5wc7G",
        "outputId": "9a9ee63c-aca5-4fc5-f34a-272c2513a5aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New data appended and FAISS index updated!\n"
          ]
        }
      ],
      "source": [
        "# Load the existing FAISS index\n",
        "load_path = \"/content/drive/MyDrive/fasis_db/faiss_index\"\n",
        "vectore_store_FAISS = FAISS.load_local(load_path, embeddings=embed_model, allow_dangerous_deserialization=True)\n",
        "\n",
        "# New document to append\n",
        "new_doc = doc #Document(page_content=\"This is new content.\", metadata={\"source\": \"new_source\"})\n",
        "\n",
        "# Add the new document to the FAISS index\n",
        "vectore_store_FAISS.add_documents([new_doc])\n",
        "\n",
        "# Save the updated index\n",
        "vectore_store_FAISS.save_local(load_path)\n",
        "\n",
        "print(\"New data appended and FAISS index updated!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKqRJDV-dGce"
      },
      "outputs": [],
      "source": [
        "# Path to the saved FAISS index\n",
        "load_path = \"/content/drive/MyDrive/fasis_db/faiss_index\"\n",
        "embed_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "# Load the FAISS vector store\n",
        "vectore_store_FAISS_db = FAISS.load_local(load_path, embeddings=embed_model,allow_dangerous_deserialization=True)\n",
        "\n",
        "print(\"FAISS index loaded successfully!\")\n",
        "# Example to query the vector store\n",
        "query = \"experienced in jango and flask and  Kubernetes Jenkins\"\n",
        "results = vectore_store_FAISS_db.similarity_search(query,k=2)\n",
        "\n",
        "for result in results:\n",
        "    print(f\"Metadata: {result.metadata}\")\n",
        "    print(f\"Document Content: {result.page_content}\")\n",
        "    #print(f\"Similarity Score: {result.score}\")  # Print the similarity score\n",
        "\n",
        "    print(\"----\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duwB92EBSw5S"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHe-xIXAFn84",
        "outputId": "db8d60fb-6aa8-46d2-f3f4-d13a87a1c401"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PineconeEmbeddings(model='multilingual-e5-large', batch_size=96, query_params={'input_type': 'query', 'truncation': 'END'}, document_params={'input_type': 'passage', 'truncation': 'END'}, dimension=1024, show_progress_bar=True, pinecone_api_key=SecretStr('**********'))"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "from langchain_pinecone import PineconeEmbeddings\n",
        "from langchain_community.vectorstores import Pinecone\n",
        "os.environ[\"PINECONE_API_KEY\"] = \"pcsk_bx41y_64e6FxaTR6A1StMSez4LYUKZJ69HzYyEUW6CvU9TuqCuW3UFRuWqJGhyhg67AaD\"\n",
        "model = PineconeEmbeddings(model=\"multilingual-e5-large\",show_progress_bar=True)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vN3ZBG3yK2EI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_community.chat_models import ChatOpenAI\n",
        "from langchain_community.embeddings import CohereEmbeddings\n",
        "from langchain_community.vectorstores import Pinecone\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
        "from pinecone import Pinecone as PineconeClient\n",
        "# import requests\n",
        "# from pinecone.core.client import ServerlessSpec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giNx9k8u3WVh"
      },
      "outputs": [],
      "source": [
        "PINECONE_API_KEY=os.environ[\"PINECONE_API_KEY\"]\n",
        "PINECONE_ENVIRONMENT=os.environ[\"PINECONE_ENVIRONMENT\"]\n",
        "PINECONE_INDEX_NAME=os.environ[\"PINECONE_INDEX_NAME\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbpxDLKh5F79"
      },
      "outputs": [],
      "source": [
        "! pip install pinecone-client -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iXKGc0P3zIO"
      },
      "outputs": [],
      "source": [
        "import pinecone\n",
        "from langchain.vectorstores import Pinecone\n",
        "from langchain.embeddings import PineconeEmbeddings\n",
        "#from langchain.embeddings.pinecone import PineconeEmbeddings\n",
        "from langchain.retrievers import PineconeHybridSearchRetriever\n",
        "from rank_bm25 import BM25Okapi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRTYLtmlEUJ4"
      },
      "outputs": [],
      "source": [
        "original_doc1=docs1\n",
        "original_doc1=[original_doc1]\n",
        "original_doc1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlBoCX5PLG6s"
      },
      "outputs": [],
      "source": [
        "\n",
        "# docs=[doc]\n",
        "\n",
        "pinecone = PineconeClient(api_key=PINECONE_API_KEY,\n",
        "                          environment=PINECONE_ENVIRONMENT)\n",
        "index_name = PINECONE_INDEX_NAME\n",
        "embeddings = PineconeEmbeddings(model=\"multilingual-e5-large\")\n",
        "docs_to_vectorstore = PineconeVectorStore.from_documents(\n",
        "          documents=original_doc,\n",
        "          index_name=\"resume-retrival\",\n",
        "          embedding=embeddings\n",
        "      )\n",
        "#\n",
        "# Define BM25 Sparse Encoder (using the rank_bm25 package)\n",
        "tokenized_corpus = [doc.page_content.lower().split() for doc in docs]\n",
        "bm25 = BM25Okapi(tokenized_corpus)\n",
        "\n",
        "class BM25SparseEncoder:\n",
        "    def __init__(self, bm25):\n",
        "        self.bm25 = bm25\n",
        "\n",
        "    def encode(self, query):\n",
        "        \"\"\"Compute BM25 scores for a given query\"\"\"\n",
        "        tokenized_query = query.lower().split()\n",
        "        scores = self.bm25.get_scores(tokenized_query)\n",
        "        return {i: score for i, score in enumerate(scores) if score > 0}\n",
        "\n",
        "# Instantiate the BM25 Sparse Encoder\n",
        "sparse_encoder = BM25SparseEncoder(bm25)\n",
        "\n",
        "# # Set up Hybrid Search Retriever\n",
        "retriever = PineconeHybridSearchRetriever(\n",
        "    vectorstore=docs_to_vectorstore,\n",
        "    sparse_encoder=sparse_encoder,\n",
        "    alpha=0.5\n",
        ")\n",
        "# Perform a query\n",
        "query = \"micro services and flask and python\"\n",
        "results = retriever.get_relevant_documents(query, top_k=3)\n",
        "\n",
        "# Display results\n",
        "for idx, doc in enumerate(results):\n",
        "    print(f\"Result {idx + 1}:\")\n",
        "    print(f\"Metadata: {doc.metadata}\")  # Extract metadata\n",
        "    print(f\"Content: {doc.page_content[:1000]}\")  # Display first 1000 characters of content\n",
        "    print(\"-\" * 80)  # Separator for better readability\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzxjnHn6yy7w"
      },
      "outputs": [],
      "source": [
        "from rank_bm25 import BM25Okapi\n",
        "# Tokenize documents\n",
        "tokenized_corpus = [doc.lower().split() for doc in original_doc]\n",
        "\n",
        "# Initialize BM25 model\n",
        "bm25 = BM25Okapi(tokenized_corpus)\n",
        "\n",
        "# Define Sparse Encoder (BM25)\n",
        "class BM25SparseEncoder:\n",
        "    def __init__(self, bm25):\n",
        "        self.bm25 = bm25\n",
        "\n",
        "    def encode(self, query):\n",
        "        \"\"\"Compute BM25 scores for a given query\"\"\"\n",
        "        tokenized_query = query.lower().split()\n",
        "        scores = self.bm25.get_scores(tokenized_query)\n",
        "        return {i: score for i, score in enumerate(scores) if score > 0}\n",
        "\n",
        "# Instantiate Sparse Encoder\n",
        "sparse_encoder = BM25SparseEncoder(bm25)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVacaU8Ezbrl"
      },
      "outputs": [],
      "source": [
        "# Assuming `original_doc` is a list of documents you want to insert into Pinecone\n",
        "docs_to_vectorstore = PineconeVectorStore.from_documents(\n",
        "    documents=original_doc,\n",
        "    index_name=\"resume-retrival\",\n",
        "    embedding=embeddings\n",
        ")\n",
        "\n",
        "# Initialize Hybrid Search Retriever\n",
        "retriever = PineconeHybridSearchRetriever(\n",
        "    embeddings=embeddings,\n",
        "    vectorstore=docs_to_vectorstore,\n",
        "    sparse_encoder=sparse_encoder,  # Use BM25 or TF-IDF encoder here\n",
        "    alpha=0.5,  # Balance between dense and sparse search\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "az0DmEUvzyN2"
      },
      "outputs": [],
      "source": [
        "query = \"micro services and flask and python\"\n",
        "results = retriever.get_relevant_documents(query, top_k=2)\n",
        "\n",
        "for idx, doc in enumerate(results):\n",
        "    print(f\"Result {idx + 1}:\")\n",
        "    print(f\"Metadata: {doc.metadata}\")  # Extract metadata\n",
        "    print(f\"Content: {doc.page_content[:1000]}\")  # Display first 1000 characters\n",
        "    print(\"-\" * 80)  # Separator for better readability\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WiaTCCbQEXn"
      },
      "outputs": [],
      "source": [
        "\n",
        "query = \"micro services and flask and python\"\n",
        "# Perform the vector search\n",
        "vector_search_results = docs_to_vectorstore.similarity_search(query,k=2)\n",
        "\n",
        "for idx, doc in enumerate(vector_search_results):\n",
        "    print(f\"Result {idx + 1}:\")\n",
        "    print(f\"Metadata: {doc.metadata}\")  # Extract metadata\n",
        "    print(f\"Content: {doc.page_content[:1000]}\")  # Display first 500 characters of the content\n",
        "    print(\"-\" * 80)  # Separator for better readability"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ICv6v-ehHKCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wU5rz4LW38hS"
      },
      "outputs": [],
      "source": [
        "# @ CHat Bot gen-AI\n",
        "## CHat Bot gen-A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5Y1RvEzIZU9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dbd8415-d3f9-4ce9-f1bc-4ef619cd40d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "! pip install --upgrade langchain langchain-community groq -q\n",
        "! pip install langchain-groq -q\n",
        "! pip install pymupdf -q\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b73tp8413-WV"
      },
      "outputs": [],
      "source": [
        "os.environ[\"GROQ_API_KEY\"] = \"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\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.chat_models import ChatOllama\n",
        "from langchain_community.embeddings import OllamaEmbeddings\n",
        "from langchain_community import embeddings\n",
        "from langchain.schema import Document\n",
        "from langchain.vectorstores import FAIS"
      ],
      "metadata": {
        "id": "FuyvWvy1F6pJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKWV7WqWIdza",
        "outputId": "8f3a5335-b01f-49c9-c067-ff3dd5cb4960"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uH8BXVILBRUl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "from dotenv import load_dotenv\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.schema import SystemMessage, HumanMessage\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema import SystemMessage\n",
        "import json\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.document_loaders import PyMuPDFLoader  # PDF Loader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ✅ Step 1: Load PDF and Split Pages as Single Chunks\n",
        "pdf_path = \"/content/drive/MyDrive/g-nayan_vector_db/G-Nayan- DR AI Soln - Project Review Doc_Ethics Committe_Gandhi Medical College (1).pdf\"  # Replace with your actual PDF path\n",
        "loader = PyMuPDFLoader(pdf_path)  # Load PDF\n",
        "documents = loader.load()  # Extract all pages as separate documents\n",
        "\n",
        "# ✅ Step 2: Assign Metadata (Page Number)\n",
        "for i, doc in enumerate(documents):\n",
        "    doc.metadata[\"page_number\"] = i + 1  # Track page numbers\n",
        "\n",
        "# ✅ Step 3: Initialize HuggingFace Embeddings\n",
        "embed_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "\n",
        "# ✅ Step 4: Store in FAISS Vector Database\n",
        "vector_store_FAISS = FAISS.from_documents(documents, embedding=embed_model)\n",
        "\n",
        "# ✅ Step 5: Save FAISS Index\n",
        "faiss_save_path = \"/content/drive/MyDrive/g-nayan_vector_db/dr_index_faiss\"\n",
        "\n",
        "vector_store_FAISS.save_local(faiss_save_path)\n",
        "print(f\"FAISS index saved to {faiss_save_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkY0LIHaIVPH",
        "outputId": "afdc0450-af95-4832-c133-23ef43c8de3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAISS index saved to /content/drive/MyDrive/g-nayan_vector_db/dr_index_faiss\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# ✅ Step 1: Define Paths\n",
        "faiss_save_path = \"/content/drive/MyDrive/g-nayan_vector_db/dr_index_faiss\"\n",
        "new_pdf_path = \"/content/drive/MyDrive/g-nayan_vector_db/G-Nayan- DR AI Soln - Project Review Doc_Ethics Committe_Gandhi Medical College (1).pdf\"  # Update with actual path\n",
        "\n",
        "# ✅ Step 2: Initialize Embedding Model\n",
        "embed_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "\n",
        "# ✅ Step 3: Load Existing FAISS Index (If Exists)\n",
        "if os.path.exists(faiss_save_path):\n",
        "    try:\n",
        "        vector_store_FAISS = FAISS.load_local(faiss_save_path, embed_model, allow_dangerous_deserialization=True)\n",
        "        print(\"✅ Loaded existing FAISS index.\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error loading FAISS index: {e}\")\n",
        "        vector_store_FAISS = None\n",
        "else:\n",
        "    print(\"❌ No existing FAISS index found. A new one will be created.\")\n",
        "    vector_store_FAISS = None\n",
        "\n",
        "# ✅ Step 4: Load New PDF and Process It\n",
        "new_loader = PyMuPDFLoader(new_pdf_path)  # Load new PDF\n",
        "new_documents = new_loader.load()  # Extract pages\n",
        "\n",
        "# Assign metadata (Page Number)\n",
        "for i, doc in enumerate(new_documents):\n",
        "    doc.metadata[\"page_number\"] = i + 1  # Adjust as needed\n",
        "\n",
        "# ✅ Step 5: Convert New Documents to FAISS Vectors\n",
        "new_vector_store = FAISS.from_documents(new_documents, embedding=embed_model)\n",
        "\n",
        "# ✅ Step 6: Merge with Existing FAISS Index (if available)\n",
        "if vector_store_FAISS:\n",
        "    vector_store_FAISS.merge_from(new_vector_store)\n",
        "else:\n",
        "    vector_store_FAISS = new_vector_store  # If no existing index, use the new one\n",
        "\n",
        "# ✅ Step 7: Save Updated FAISS Index\n",
        "vector_store_FAISS.save_local(faiss_save_path)\n",
        "print(f\"✅ FAISS index updated and saved to {faiss_save_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0qMnpBRNsjk",
        "outputId": "b318b840-1232-48ab-bfe5-72b72f1ef56b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Loaded existing FAISS index.\n",
            "✅ FAISS index updated and saved to /content/drive/MyDrive/g-nayan_vector_db/dr_index_faiss\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Load the FAISS Index\n",
        "vector_store_FAISS = FAISS.load_local(faiss_save_path, embed_model,allow_dangerous_deserialization=True) # ✅ Enable safe deserialization)\n",
        "\n",
        "# ✅ Query Example\n",
        "query = \"g-nayana\"\n",
        "query_embedding = embed_model.embed_query(query)\n",
        "\n",
        "# ✅ Search FAISS for Similar Chunks\n",
        "retrieved_docs = vector_store_FAISS.similarity_search(query, k=3)\n",
        "\n",
        "\n",
        "# ✅ Print Retrieved Documents\n",
        "for doc in retrieved_docs:\n",
        "    print(f\"Page{doc.page_content}\\n\")\n"
      ],
      "metadata": {
        "id": "Q_xhwAOFK5O_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install rank-bm25 -q"
      ],
      "metadata": {
        "id": "Qif4PgTBQ0UI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "_z9AplcFUjOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rank_bm25 import BM25Okapi\n",
        "from langchain.vectorstores import FAISS\n",
        "embed_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "faiss_save_path = \"/content/drive/MyDrive/g-nayan_vector_db/dr_index_faiss\"\n",
        "# ✅ Load FAISS Vector Store\n",
        "vector_store_FAISS = FAISS.load_local(faiss_save_path, embed_model, allow_dangerous_deserialization=True)\n",
        "\n",
        "# ✅ Load Text Data for BM25 (Assuming 'documents' is a list of text chunks)\n",
        "documents = [doc.page_content for doc in vector_store_FAISS.docstore._dict.values()]\n",
        "tokenized_docs = [doc.split() for doc in documents]  # Tokenize for BM25\n",
        "bm25 = BM25Okapi(tokenized_docs)\n",
        "\n",
        "# ✅ Hybrid Search Function\n",
        "def hybrid_search(query, k=3, lambda_weight=0.5):\n",
        "    query_embedding = embed_model.embed_query(query)\n",
        "\n",
        "    # ✅ FAISS Dense Search\n",
        "    dense_results = vector_store_FAISS.similarity_search(query, k=k)\n",
        "\n",
        "    # ✅ BM25 Sparse Search\n",
        "    bm25_scores = bm25.get_scores(query.split())\n",
        "    top_k_bm25 = sorted(range(len(bm25_scores)), key=lambda i: bm25_scores[i], reverse=True)[:k]\n",
        "    sparse_results = [documents[i] for i in top_k_bm25]\n",
        "\n",
        "    # ✅ Combine Results (Weighted Ranking)\n",
        "    hybrid_results = []\n",
        "    for i in range(k):\n",
        "        dense_score = (1 - lambda_weight) * (k - i)  # Higher rank = higher weight\n",
        "        sparse_score = lambda_weight * (k - i)  # Higher rank = higher weight\n",
        "        hybrid_results.append((dense_results[i].page_content, dense_score))\n",
        "        hybrid_results.append((sparse_results[i], sparse_score))\n",
        "\n",
        "    # ✅ Sort & Return Final Ranked Results\n",
        "    hybrid_results.sort(key=lambda x: x[1], reverse=True)\n",
        "    return [res[0] for res in hybrid_results[:k]]\n",
        "\n",
        "# ✅ Query Example\n",
        "query = user_input#\"what is G-nayan or g-nayana\"\n",
        "results = hybrid_search(query, k=5)\n",
        "# results\n",
        "# # # ✅ Print Retrieved Documents\n",
        "# # for idx, res in enumerate(results):\n",
        "# #     print(f\"{idx+1}. {res}\\n\")\n",
        "# context_text = \"\\n\\n\".join(results)\n",
        "# context_text\n",
        "load_dotenv()\n",
        "api_key = os.getenv(\"GROQ_API_KEY\")\n",
        "# Initialize Memory with Window Size 3\n",
        "context_text = \"\\n\\n\".join(results)\n",
        "if not api_key:\n",
        "    api_key = getpass.getpass(\"Enter API key for Groq: \")\n",
        "    os.environ[\"GROQ_API_KEY\"] = api_key  # Store it in the environment\n",
        "\n",
        "# Initialize the Groq chat model\n",
        "chat_model = ChatGroq(model_name=\"llama3-8b-8192\", temperature=0.3, max_tokens=200)\n",
        "system_prompt_template = PromptTemplate.from_template(\n",
        "    \"Summarize the following documents relevant to the query{context} less than 200 words in a good formate \"\n",
        ")\n",
        "# Create a system message from the template\n",
        "system_message = SystemMessage(content=system_prompt_template.format(context=context_text))\n",
        "\n",
        "print(\"your Assistant is ready! Type 'exit' or 'quit' to end the chat.\")\n",
        "\n",
        "# Chat loop\n",
        "while True:\n",
        "    user_input = input(\"You: \").strip()\n",
        "\n",
        "    if user_input.lower() in [\"exit\", \"quit\",\"thank you\",\"tq\"]:\n",
        "        print(\"G-Nayan: Goodbye!\")\n",
        "        break\n",
        "    # Get response from the chatbot with stored memory\n",
        "    response = chat_model.invoke([\n",
        "        system_message,\n",
        "        HumanMessage(content=user_input)\n",
        "    ])\n",
        "\n",
        "    # Store user input and bot response in memory\n",
        "    print(f\"G-Nayan: {response.content}\")"
      ],
      "metadata": {
        "id": "9apWnWvyQbCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load API key from .env file\n",
        "load_dotenv()\n",
        "api_key = os.getenv(\"GROQ_API_KEY\")\n",
        "# Initialize Memory with Window Size 3\n",
        "context_text = \"\\n\\n\".join(results)\n",
        "if not api_key:\n",
        "    api_key = getpass.getpass(\"Enter API key for Groq: \")\n",
        "    os.environ[\"GROQ_API_KEY\"] = api_key  # Store it in the environment\n",
        "\n",
        "# Initialize the Groq chat model\n",
        "chat_model = ChatGroq(model_name=\"llama3-8b-8192\", temperature=0.3, max_tokens=200)\n",
        "system_prompt_template = PromptTemplate.from_template(\n",
        "    \"Summarize the following documents relevant to the query{context} less than 200 words in a good formate \"\n",
        ")\n",
        "# Create a system message from the template\n",
        "system_message = SystemMessage(content=system_prompt_template.format(context=context_text))\n",
        "\n",
        "print(\"your Assistant is ready! Type 'exit' or 'quit' to end the chat.\")\n",
        "\n",
        "# Chat loop\n",
        "while True:\n",
        "    user_input = input(\"You: \").strip()\n",
        "\n",
        "    if user_input.lower() in [\"exit\", \"quit\",\"thank you\",\"tq\"]:\n",
        "        print(\"G-Nayan: Goodbye!\")\n",
        "        break\n",
        "    # Get response from the chatbot with stored memory\n",
        "    response = chat_model.invoke([\n",
        "        system_message,\n",
        "        HumanMessage(content=user_input)\n",
        "    ])\n",
        "\n",
        "    # Store user input and bot response in memory\n",
        "    print(f\"G-Nayan: {response.content}\")"
      ],
      "metadata": {
        "id": "_kFgHRxPW7M8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# ✅ Initialize Embedding Model\n",
        "embed_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "\n",
        "# ✅ Load FAISS Vector Store\n",
        "faiss_save_path = \"/content/drive/MyDrive/g-nayan_vector_db/dr_index_faiss\"\n",
        "vector_store_FAISS = FAISS.load_local(faiss_save_path, embed_model, allow_dangerous_deserialization=True)\n",
        "\n",
        "# ✅ Load Text Data for BM25 (Assuming 'documents' is a list of text chunks)\n",
        "documents = [doc.page_content for doc in vector_store_FAISS.docstore._dict.values()]\n",
        "tokenized_docs = [doc.split() for doc in documents]  # Tokenize for BM25\n",
        "bm25 = BM25Okapi(tokenized_docs)\n",
        "\n",
        "# ✅ Hybrid Search Function\n",
        "def hybrid_search(query, k=3, lambda_weight=0.5):\n",
        "    query_embedding = embed_model.embed_query(query)\n",
        "\n",
        "    # ✅ FAISS Dense Search\n",
        "    dense_results = vector_store_FAISS.similarity_search(query, k=k)\n",
        "\n",
        "    # ✅ BM25 Sparse Search\n",
        "    bm25_scores = bm25.get_scores(query.split())\n",
        "    top_k_bm25 = sorted(range(len(bm25_scores)), key=lambda i: bm25_scores[i], reverse=True)[:k]\n",
        "    sparse_results = [documents[i] for i in top_k_bm25]\n",
        "\n",
        "    # ✅ Combine Results (Weighted Ranking)\n",
        "    hybrid_results = []\n",
        "    for i in range(len(dense_results)):  # Avoid index errors\n",
        "        dense_score = (1 - lambda_weight) * (k - i)  # Higher rank = higher weight\n",
        "        hybrid_results.append((dense_results[i].page_content, dense_score))\n",
        "\n",
        "    for i in range(len(sparse_results)):\n",
        "        sparse_score = lambda_weight * (k - i)  # Higher rank = higher weight\n",
        "        hybrid_results.append((sparse_results[i], sparse_score))\n",
        "\n",
        "    # ✅ Sort & Return Final Ranked Results\n",
        "    hybrid_results.sort(key=lambda x: x[1], reverse=True)\n",
        "    return [res[0] for res in hybrid_results[:k]]\n",
        "\n",
        "# ✅ Initialize Groq LLM Model\n",
        "chat_model = ChatGroq(model_name=\"llama3-8b-8192\", temperature=0.3, max_tokens=200)\n",
        "\n",
        "def rag_pipeline(user_input,results):\n",
        "  user_input = input(\"\\nYou: \").strip()\n",
        "  # ✅ Perform Hybrid Search\n",
        "  results = hybrid_search(user_input, k=5)\n",
        "  context_text = \"\\n\\n\".join(results)\n",
        "\n",
        "  # ✅ System Prompt for Summarization\n",
        "  system_prompt_template = PromptTemplate.from_template(\n",
        "  \"Summarize the following documents relevant to the query:\\n\"\n",
        "  \"{context}\\n\\n\"\n",
        "  \"Provide the summary in less than 200 words, ensuring:\\n\"\n",
        "  \"- No hallucinations.\\n\"\n",
        "  \"- No repetition from the referenced document.\\n\"\n",
        "  \"- Maintain factual accuracy\"\n",
        "  )\n",
        "  system_message = SystemMessage(content=system_prompt_template.format(context=context_text))\n",
        "  response = chat_model.invoke([\n",
        "      system_message,\n",
        "      HumanMessage(content=user_input)\n",
        "  ])\n",
        "  return f\"\\nG-Nayan: {response.content}\"\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input in [\"exit\", \"quit\", \"thank you\", \"tq\", \"bye\"]:\n",
        "      print(\"G-Nayan: Goodbye! just ping me hi you wana kind of help\")\n",
        "      break  # Exit before calling chat_bot\n",
        "    response =rag_pipeline(user_input,results)\n",
        "    print(response)\n",
        "\n"
      ],
      "metadata": {
        "id": "ExqL-A3zVGxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # ✅ Load Text Data for BM25 (Assuming 'documents' is a list of text chunks)\n",
        "# documents = [doc.page_content for doc in vector_store_FAISS.docstore._dict.values()]\n",
        "# tokenized_docs = [doc.split() for doc in documents]  # Tokenize for BM25\n",
        "# bm25 = BM25Okapi(tokenized_docs)\n",
        "\n",
        "# # ✅ Hybrid Search Function\n",
        "# def hybrid_search(query, k=5, lambda_weight=0.5):\n",
        "#     query_embedding = embed_model.embed_query(query)\n",
        "\n",
        "#     # ✅ FAISS Dense Search\n",
        "#     dense_results = vector_store_FAISS.similarity_search(query, k=k)\n",
        "\n",
        "#     # ✅ BM25 Sparse Search\n",
        "#     bm25_scores = bm25.get_scores(query.split())\n",
        "#     top_k_bm25 = sorted(range(len(bm25_scores)), key=lambda i: bm25_scores[i], reverse=True)[:k]\n",
        "#     sparse_results = [documents[i] for i in top_k_bm25]\n",
        "\n",
        "#     # ✅ Combine Results (Weighted Ranking)\n",
        "#     hybrid_results = []\n",
        "#     for i in range(len(dense_results)):  # Avoid index errors\n",
        "#         dense_score = (1 - lambda_weight) * (k - i)  # Higher rank = higher weight\n",
        "#         hybrid_results.append((dense_results[i].page_content, dense_score))\n",
        "\n",
        "#     for i in range(len(sparse_results)):\n",
        "#         sparse_score = lambda_weight * (k - i)  # Higher rank = higher weight\n",
        "#         hybrid_results.append((sparse_results[i], sparse_score))\n",
        "\n",
        "#     # ✅ Sort & Return Final Ranked Results\n",
        "#     hybrid_results.sort(key=lambda x: x[1], reverse=True)\n",
        "#     return [res[0] for res in hybrid_results[:k]]"
      ],
      "metadata": {
        "id": "FGNL5HD6-hAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install fastapi uvicorn nest-asyncio pyngrok -q\n"
      ],
      "metadata": {
        "id": "adz9k7gu_6Dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-4ce-Bi_f1Q"
      },
      "outputs": [],
      "source": [
        "# Load API key from .env file\n",
        "!ngrok authtoken 2c7p2kLhZ8kcRfIsbSg1lzVWOqV_84ytnNJmQVngJaowK2CvZ\n",
        "\n",
        "load_dotenv()\n",
        "api_key = os.getenv(\"GROQ_API_KEY\")\n",
        "\n",
        "if not api_key:\n",
        "    api_key = getpass.getpass(\"Enter API key for Groq: \")\n",
        "    os.environ[\"GROQ_API_KEY\"] = api_key  # Store it in the environment\n",
        "# ✅ Initialize FastAPI\n",
        "app = FastAPI()\n",
        "# ✅ Initialize Embedding Model\n",
        "embed_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "\n",
        "# ✅ Load FAISS Vector Store\n",
        "faiss_save_path = \"/content/drive/MyDrive/g-nayan_vector_db/dr_index_faiss\"\n",
        "vector_store_FAISS = FAISS.load_local(faiss_save_path, embed_model, allow_dangerous_deserialization=True)\n",
        "\n",
        "\n",
        "def similarity_search(query,k):\n",
        "  vector_store_FAISS = FAISS.load_local(faiss_save_path, embed_model,allow_dangerous_deserialization=True) # ✅ Enable safe deserialization)\n",
        "  query_embedding = embed_model.embed_query(query)\n",
        "    # ✅ Search FAISS for Similar Chunks\n",
        "  retrieved_docs = vector_store_FAISS.similarity_search(query,k)\n",
        "  return retrieved_docs\n",
        "\n",
        "\n",
        "# ✅ Initialize Groq LLM Model\n",
        "chat_model_rag = ChatGroq(model_name=\"llama3-8b-8192\", temperature=0.3, max_tokens=250)\n",
        "\n",
        "def rag_pipeline(user_input):\n",
        "  # ✅ Perform Hybrid Search\n",
        "  results = similarity_search(user_input, k=5)\n",
        "  context_text = \"\\n\\n\".join([doc.page_content for doc in results])\n",
        "\n",
        "  system_prompt_template = PromptTemplate.from_template(\n",
        "    \"Summarize the following documents relevant to the query:\\n\"\n",
        "    \"{context}\\n\\n\"\n",
        "    \"Provide the summary in less than 250 words, ensuring:\\n\"\n",
        "    \"- No hallucinations.\\n\"\n",
        "    \"- No repetition from the referenced document.\\n\"\n",
        "    \"- Maintain factual accuracy.\\n\"\n",
        "    \"- If there is no similarity between query and generation, just return 'non'.\"\n",
        ")\n",
        "  system_message = SystemMessage(content=system_prompt_template.format(context=context_text))\n",
        "  response = chat_model_rag.invoke([\n",
        "      system_message,\n",
        "      HumanMessage(content=user_input)\n",
        "  ])\n",
        "  return f\"\\nG-Nayan: {response.content}\"\n",
        "report = {\n",
        "    \"patient_id\": \"ys jagan 420\",\n",
        "    \"age\":45,\n",
        "    \"phone _number\":7832697672,\n",
        "    \"email_id\":\"gandhi_hospital@gmail.com\",\n",
        "    \"left_eye\": {\n",
        "        #\"predicted_class\": 1,\n",
        "        \"stage\": \"Mild Diabetic Retinopathy\",\n",
        "        # \"confidence\": 96.52,\n",
        "        \"explanation\": \"Mild NPDR (Nonproliferative Diabetic Retinopathy):\\n Microaneurysms have been detected, which are small, localized dilations of blood vessels in the retina this finding suggests a moderate chance of progression if not monitored and managed properly.'\",\n",
        "        \"note\": \"Your eye is in the sslightly effected.\"\n",
        "    },\n",
        "    \"right_eye\": {\n",
        "        #\"predicted_class\": 3,\n",
        "        \"stage\": \"Proliferative Retinopathy\",\n",
        "        # \"confidence\": 97.66,\n",
        "        \"explanation\": \"Proliferative Diabetic Retinopathy (PDR)Neovascularization has been observed, characterized by the growth of new, abnormal blood vessels on the retina Additionally, cotton wool spots may be present, indicating localized retinal ischemia These findings carry a significant risk of vision loss and complications, necessitating urgent medical intervention\",\n",
        "        \"note\": \"Take care of your eye.\"\n",
        "    }}\n",
        "# ✅ Convert the report to a JSON string for display\n",
        "report_json = json.dumps(report, indent=2)\n",
        "def chat_bot(user_input):\n",
        "  user_input = user_input.strip()\n",
        "  # Initialize the Groq chat model\n",
        "  chat_model = ChatGroq(model_name=\"llama3-8b-8192\", temperature=0.3, max_tokens=300)\n",
        "  system_prompt_template = PromptTemplate.from_template(\n",
        "    \"You are an AI chatbot called G-Nayan. \"\n",
        "    \"You specialize in explaining diabetic retinopathy in detail. \"\n",
        "\n",
        "    \"If the user greets you, ask them if they want an analysis of their medical report. \"\n",
        "    \"If they respond with 'yes' or 'ha', analyze the provided medical report and summarize \"\n",
        "    \"both the left and right eye conditions. Provide better insights, ensuring they are not just a repetition of the report. \"\n",
        "    \"Mention the patient's name (from patient_id) and the hospital in a structured format. \"\n",
        "    \"This analysis should only be given once unless the user explicitly asks for it again.\\n\\n\"\n",
        "\n",
        "    \"Additionally, based on the report analysis, suggest:\\n\"\n",
        "    \"- Suitable food recommendations.\\n\"\n",
        "    \"- Symptoms to monitor.\\n\"\n",
        "    \"- The type of doctor the patient should consult.\\n\\n\"\n",
        "\n",
        "    \"Report Details:\\n{report}\\n\\n\"\n",
        "\n",
        "    \"Rules:\\n\"\n",
        "    \"- Never go out of your role.\\n\"\n",
        "    \"- Do not explicitly mention that you are an AI unless asked.\\n\"\n",
        "    \"- Do not repeatedly state 'As a specialist in diabetic retinopathy.'\\n\"\n",
        "    \"- If asked an inappropriate or unrelated question, respond with: \"\n",
        "    \"'I am limited to medical diabetic retinopathy discussions only.'\"\n",
        ")\n",
        "\n",
        "  # Create a system message from the template\n",
        "  system_message = SystemMessage(content=system_prompt_template.format(report=report_json))\n",
        "  # Get response from the chatbot with stored memory\n",
        "  response = chat_model.invoke([\n",
        "      system_message,\n",
        "      HumanMessage(content=user_input)\n",
        "  ])\n",
        "  return f\"\\nG-Nayan: {response.content}\"\n",
        "# ✅ Define WebSocket Endpoint\n",
        "@app.websocket(\"/chat\")\n",
        "async def websocket_endpoint(websocket: WebSocket):\n",
        "    await websocket.accept()\n",
        "    while True:\n",
        "        try:\n",
        "            user_input = await websocket.receive_text()\n",
        "            if user_input.lower() in [\"exit\", \"quit\", \"thank you\", \"bye\"]:\n",
        "                await websocket.send_text(\"G-Nayan: Goodbye! Ping me 'hi' if you need help.\")\n",
        "                break\n",
        "\n",
        "            # 🔹 Handle RAG Queries\n",
        "            if any(keyword in user_input for keyword in [\"documentation\", \"document\", \"doc\"]):\n",
        "                response = rag_pipeline(user_input)\n",
        "                if not response or response.strip().lower() == \"non\":\n",
        "                    response = chat_bot(user_input)\n",
        "            else:\n",
        "                response = chat_bot(user_input)\n",
        "\n",
        "            await websocket.send_text(f\"G-Nayan: {response}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            await websocket.send_text(f\"G-Nayan: An error occurred: {str(e)}\")\n",
        "            break\n",
        "\n",
        "    await websocket.close()\n",
        "# Start ngrok tunnel\n",
        "# ✅ Start ngrok tunnel\n",
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "print(f\"Public WebSocket URL: {ngrok_tunnel.public_url}/chat\")\n",
        "\n",
        "# ✅ Run FastAPI Server\n",
        "if __name__ == \"__main__\":\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
        "\n",
        "\n",
        "# while True:\n",
        "#     user_input = input(\"You: \").strip().lower()\n",
        "\n",
        "#     if user_input in [\"exit\", \"quit\", \"thank you\", \"tq\", \"bye\"]:\n",
        "#         print(\"G-Nayan: Goodbye! Just ping me 'hi' if you need any help.\")\n",
        "#         break\n",
        "\n",
        "#     # 🔹 Step 1: Check if the query is related to documents\n",
        "#     if any(keyword in user_input for keyword in [\"documentation\", \"document\", \"doc\"]):\n",
        "#         response = rag_pipeline(user_input)\n",
        "\n",
        "#         # 🔹 Step 2: If RAG fails (returns empty or \"non\"), fallback to chatbot\n",
        "#         if not response or response.strip().lower() == \"non\":\n",
        "#             response = chat_bot(user_input)\n",
        "#     else:\n",
        "#         # 🔹 Default to chatbot for general queries\n",
        "#         response = chat_bot(user_input)\n",
        "\n",
        "#     print(response)\n",
        "\n",
        "\n",
        "\n",
        "# # ✅ Define tools for Langchain\n",
        "# chatbot_tool = Tool(\n",
        "#     name=\"ChatBot\",\n",
        "#     func=lambda query: chat_bot(query),\n",
        "#     description=\"Use for general conversational queries like greetings and normal conversations. hi or hello general questions\"\n",
        "# )\n",
        "# rag_tool = Tool(\n",
        "#     name=\"RAG Retrieval\",\n",
        "#     func=lambda query: rag_pipeline(query),\n",
        "#     description=\"Use when a user asks for factual or document-based information retrieved from the RAG pipeline.\"\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken 2c7p2kLhZ8kcRfIsbSg1lzVWOqV_84ytnNJmQVngJaowK2CvZ\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cjz4u13vF_zw",
        "outputId": "097214bb-54a8-4685-de43-ca52133fae39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ✅ Load environment variables\n",
        "load_dotenv()\n",
        "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
        "api_key = os.getenv(\"GROQ_API_KEY\")\n",
        "\n",
        "if not api_key:\n",
        "    api_key = getpass.getpass(\"Enter API key for Groq: \")\n",
        "    os.environ[\"GROQ_API_KEY\"] = api_key  # Store it in the environment\n",
        "\n",
        "# ✅ Initialize Embedding Model\n",
        "embed_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "\n",
        "# ✅ Load FAISS Vector Store\n",
        "faiss_save_path = \"/content/drive/MyDrive/g-nayan_vector_db/dr_index_faiss\"\n",
        "vector_store_FAISS = FAISS.load_local(faiss_save_path, embed_model, allow_dangerous_deserialization=True)\n",
        "\n",
        "def similarity_search(query, k):\n",
        "    vector_store_FAISS = FAISS.load_local(faiss_save_path, embed_model, allow_dangerous_deserialization=True)\n",
        "    query_embedding = embed_model.embed_query(query)\n",
        "    retrieved_docs = vector_store_FAISS.similarity_search(query, k)\n",
        "    return retrieved_docs\n",
        "\n",
        "# ✅ Initialize Groq LLM Model\n",
        "chat_model_rag = ChatGroq(model_name=\"llama3-8b-8192\", temperature=0.3, max_tokens=250)\n",
        "\n",
        "def rag_pipeline(user_input):\n",
        "    results = similarity_search(user_input, k=5)\n",
        "    context_text = \"\\n\\n\".join([doc.page_content for doc in results])\n",
        "\n",
        "    system_prompt_template = PromptTemplate.from_template(\n",
        "        \"Summarize the following documents relevant to the query:\\n\"\n",
        "        \"{context}\\n\\n\"\n",
        "        \"Provide the summary in less than 250 words, ensuring:\\n\"\n",
        "        \"- No hallucinations.\\n\"\n",
        "        \"- No repetition from the referenced document.\\n\"\n",
        "        \"- Maintain factual accuracy.\\n\"\n",
        "        \"- If there is no similarity between query and generation, just return 'non'.\"\n",
        "    )\n",
        "    system_message = SystemMessage(content=system_prompt_template.format(context=context_text))\n",
        "    response = chat_model_rag.invoke([\n",
        "        system_message,\n",
        "        HumanMessage(content=user_input)\n",
        "    ])\n",
        "    return f\"\\nG-Nayan: {response.content}\"\n",
        "\n",
        "# ✅ Define patient report\n",
        "report = {\n",
        "    \"patient_id\": \"ys jagan 420\",\n",
        "    \"age\": 45,\n",
        "    \"phone_number\": 7832697672,\n",
        "    \"email_id\": \"gandhi_hospital@gmail.com\",\n",
        "    \"left_eye\": {\n",
        "        \"stage\": \"Mild Diabetic Retinopathy\",\n",
        "        \"explanation\": \"Mild NPDR (Nonproliferative Diabetic Retinopathy):\\n Microaneurysms have been detected, which are small, localized dilations of blood vessels in the retina. This finding suggests a moderate chance of progression if not monitored and managed properly.\",\n",
        "        \"note\": \"Your eye is slightly affected.\"\n",
        "    },\n",
        "    \"right_eye\": {\n",
        "        \"stage\": \"Proliferative Retinopathy\",\n",
        "        \"explanation\": \"Proliferative Diabetic Retinopathy (PDR): Neovascularization has been observed, characterized by the growth of new, abnormal blood vessels on the retina. Additionally, cotton wool spots may be present, indicating localized retinal ischemia. These findings carry a significant risk of vision loss and complications, necessitating urgent medical intervention.\",\n",
        "        \"note\": \"Take care of your eye.\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# ✅ Convert the report to a JSON string for display\n",
        "report_json = json.dumps(report, indent=2)\n",
        "\n",
        "def chat_bot(user_input):\n",
        "    user_input = user_input.strip()\n",
        "    chat_model = ChatGroq(model_name=\"llama3-8b-8192\", temperature=0.3, max_tokens=300)\n",
        "\n",
        "    system_prompt_template = PromptTemplate.from_template(\n",
        "        \"You are an AI chatbot called G-Nayan. \"\n",
        "        \"You specialize in explaining diabetic retinopathy in detail. \"\n",
        "        \"If the user greets you, ask them if they want an analysis of their medical report. \"\n",
        "        \"If they respond with 'yes' or 'ha', analyze the provided medical report and summarize \"\n",
        "        \"both the left and right eye conditions. Provide better insights, ensuring they are not just a repetition of the report. \"\n",
        "        \"Mention the patient's name (from patient_id) and the hospital in a structured format. \"\n",
        "        \"This analysis should only be given once unless the user explicitly asks for it again.\\n\\n\"\n",
        "        \"Additionally, based on the report analysis, suggest:\\n\"\n",
        "        \"- Suitable food recommendations.\\n\"\n",
        "        \"- Symptoms to monitor.\\n\"\n",
        "        \"- The type of doctor the patient should consult.\\n\\n\"\n",
        "        \"Report Details:\\n{report}\\n\\n\"\n",
        "        \"Rules:\\n\"\n",
        "        \"- Never go out of your role.\\n\"\n",
        "        \"- Do not explicitly mention that you are an AI unless asked.\\n\"\n",
        "        \"- Do not repeatedly state 'As a specialist in diabetic retinopathy.'\\n\"\n",
        "        \"- If asked an inappropriate or unrelated question, respond with: \"\n",
        "        \"'I am limited to medical diabetic retinopathy discussions only.'\"\n",
        "    )\n",
        "    system_message = SystemMessage(content=system_prompt_template.format(report=report_json))\n",
        "    response = chat_model.invoke([\n",
        "        system_message,\n",
        "        HumanMessage(content=user_input)\n",
        "    ])\n",
        "    return f\"\\nG-Nayan: {response.content}\"\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.agents import AgentType\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# ✅ Define tools for Langchain\n",
        "chatbot_tool = Tool(\n",
        "    name=\"ChatBot\",\n",
        "    func=chat_bot,\n",
        "      description=(\n",
        "        \"Use this tool for general conversational queries like greetings, normal discussions, \"\n",
        "        \"or when the user asks for an analysis of their diabetic retinopathy report. \"\n",
        "        \"This tool is responsible for summarizing patient reports and providing relevant insights.\"\n",
        "    ))\n",
        "\n",
        "rag_tool = Tool(\n",
        "    name=\"RAG Retrieval\",\n",
        "    func=rag_pipeline,\n",
        "     description=(\n",
        "        \"Use this tool when the user asks for document-based information from the RAG pipeline. \"\n",
        "        \"It is best suited for answering queries about 'G-Nayan', 'Diabetic Retinopathy', \"\n",
        "        \"or when searching for food recommendations and doctors related to diabetic retinopathy.\"\n",
        "    ))\n",
        "llm = ChatGroq(model_name=\"llama3-8b-8192\", temperature=0.3, max_tokens=300)\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "\n",
        "# ✅ Create an agent\n",
        "agent = initialize_agent(\n",
        "    llm=llm,\n",
        "    tools=[chatbot_tool,rag_tool],  # Add both tools\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,  # Allows the agent to decide based on descriptions\n",
        "    verbose=False,\n",
        "    max_execution_time=25,\n",
        "    memory=memory\n",
        ")\n",
        "\n",
        "# ✅ Chat loop using the agent\n",
        "while True:\n",
        "    user_input = input(\"You: \").strip().lower()\n",
        "\n",
        "    if user_input in [\"exit\", \"quit\", \"thank you\", \"tq\", \"bye\"]:\n",
        "        print(\"G-Nayan: Goodbye! Just ping me 'hi' if you need any help.\")\n",
        "        break\n",
        "\n",
        "    # 🔹 Let the agent handle tool selection\n",
        "    response = agent.run(user_input)\n",
        "\n",
        "    print(f\"\\nG-Nayan: {response}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "V9Xx3YZdl7BU",
        "outputId": "bc8a528c-d052-4cb6-c8e2-6c1c950686b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: hii\n",
            "\n",
            "G-Nayan: Agent stopped due to iteration limit or time limit.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-9c571333bf08>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;31m# ✅ Chat loop using the agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"exit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"quit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"thank you\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tq\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bye\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "re"
      ],
      "metadata": {
        "id": "Y3jrLO1AFeGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for_query = ChatGroq(model_name=\"llama3-8b-8192\", temperature=0.0, max_tokens=100)\n",
        "\n",
        "def llm_query_rewrite(query):\n",
        "    \"\"\"Rewrite user queries using LLM for better retrieval performance.\"\"\"\n",
        "    prompt = f\"Rewrite the following search query in a well-structured, concise format:\\n\\n'{query}'\"\n",
        "\n",
        "    try:\n",
        "        response = for_query.invoke(prompt)\n",
        "        return response.content # Ensure clean output\n",
        "    except Exception as e:\n",
        "        print(f\"LLM Query Rewrite Error: {str(e)}\")  # Log errors\n",
        "        return query  # Fall back to original query if LLM fails\n",
        "        print(response.content)\n",
        "llm_query_rewrite(\"what is diabeeetic retinopthy\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "wICZZpGRz5tB",
        "outputId": "c4452ab4-e43a-42b8-9ef4-f491cddf24aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Here is a rewritten version of the search query in a well-structured and concise format:\\n\\n\"Definition of Diabetic Retinopathy\"\\n\\nor\\n\\n\"What is Diabetic Retinopathy?\"\\n\\nor\\n\\n\"Information on Diabetic Retinopathy\"\\n\\nThis rewritten query is more specific and clear, making it easier for search engines to understand what you\\'re looking for.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for_query = ChatGroq(model_name=\"llama3-8b-8192\", temperature=0.0, max_tokens=50)\n",
        "def llm_query_rewrite(query):\n",
        "    \"\"\"Rewrite user queries using LLM for better retrieval performance.\"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    Rewrite the following search query in a well-structured, concise format.\n",
        "    Directly return **only** 2-3 query variations, with each query on a new line.\n",
        "    No explanations, no numbering, no extra words.\n",
        "\n",
        "    Input: {query}\n",
        "    Output:\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = for_query.invoke(prompt)\n",
        "\n",
        "        # ✅ Ensure we extract text correctly\n",
        "        if hasattr(response, \"content\") and isinstance(response.content, str):\n",
        "            queries = response.content.strip().split(\"\\n\")\n",
        "            return [q.strip() for q in queries if q.strip()]  # Clean spaces and blank lines\n",
        "\n",
        "        return [query]  # Fallback to original query\n",
        "    except Exception as e:\n",
        "        print(f\"LLM Query Rewrite Error: {str(e)}\")\n",
        "        return [query]  # Fallback to original query\n",
        "\n",
        "# 🔍 Test case\n",
        "# cleaned_queries = llm_query_rewrite(\"what are foods to be taken for overcoming daibatic retinopathy\")\n",
        "# cleaned_queries\n",
        "\n",
        "def similarity_search(query, k):\n",
        "    # Load FAISS index\n",
        "    vector_store_FAISS = FAISS.load_local(faiss_save_path, embed_model,allow_dangerous_deserialization=True)\n",
        "\n",
        "    # Rewrite the query for better retrieval\n",
        "    cleaned_queries = llm_query_rewrite(query)\n",
        "    print(cleaned_queries)\n",
        "    # Convert the first rewritten query into an embedding\n",
        "    query_embedding = embed_model.embed_query(cleaned_queries[0])\n",
        "\n",
        "    # Perform similarity search using the query vector\n",
        "    retrieved_docs = vector_store_FAISS.similarity_search_by_vector(query_embedding, k)\n",
        "\n",
        "    return retrieved_docs\n",
        "\n",
        "# Test the function\n",
        "print(similarity_search(\"what is Diabetic renouhd\", k=3))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_Q-LaAfz5K_",
        "outputId": "c6700cb2-1084-4de3-86f7-afac83b15ece"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['what is diabetic retinopathy', 'what is diabetic retinopaty', 'diabetic retinopathy definition']\n",
            "[Document(id='eae816d1-1d28-4a2d-b415-f73b91ca61ca', metadata={'producer': 'Adobe PDF Library 11.0', 'creator': 'Adobe InDesign CC 2014 (Macintosh)', 'creationdate': '2016-08-04T12:03:10-05:00', 'source': '/content/drive/MyDrive/g-nayan_vector_db/fact_sheet_22_diabetic_retinopathy_new.pdf', 'file_path': '/content/drive/MyDrive/g-nayan_vector_db/fact_sheet_22_diabetic_retinopathy_new.pdf', 'total_pages': 4, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2016-08-04T12:03:12-05:00', 'trapped': '', 'modDate': \"D:20160804120312-05'00'\", 'creationDate': \"D:20160804120310-05'00'\", 'page': 0, 'page_number': 1}, page_content='Diabetic Retinopathy: Diabetic retinopathy \\n(pronounced ret in OP uh thee) is a complication of diabetes \\nthat causes damage to the blood vessels of the retina— \\nthe light-sensitive tissue that lines the back part of the eye, \\nallowing you to see fine detail.  \\nAmerican Society of Retina Specialists\\nThe Foundation\\nR E T I N A  H E A LT H  S E R I E S  |  Facts from the ASRS\\nCommitted to improving  \\nthe quality of life of all people  \\nwith retinal disease. \\nCopyright 2016 The Foundation of the American Society of Retina Specialists. All rights reserved.savingvision.org  I  20 North Wacker Drive, Suite 2030, Chicago, IL 60606  |  (312) 578-8760\\nDiabetic retinopathy is the most common cause of irreversible blindness in \\nworking-age Americans. As many people with type 1 diabetes suffer blindness \\nas those with the more common type 2 disease. Diabetic retinopathy occurs \\nin more than half of the people who develop diabetes. \\nCauses: The primary cause of diabetic retinopathy is diabetes—a condition in \\nwhich the levels of glucose (sugar) in the blood are too high. Elevated sugar \\nlevels from diabetes can damage the small blood vessels that nourish the  \\nretina and may, in some cases, block them completely.  \\n\\t\\nWhen damaged blood vessels leak fluid into the retina it results in a  \\ncondition known as diabetic macular edema which causes swelling in the \\ncenter part of the eye (macula) that provides the sharp vision needed for \\nreading and recognizing faces.  \\n\\t\\nProlonged damage to the small blood vessels in the retina results in poor \\ncirculation to the retina and macula prompting the development of growth \\nfactors that cause new abnormal blood vessels (neovascularization) and scar \\ntissue to grow on the surface of the retina. This stage of the disease is known \\nas proliferative diabetic retinopathy (PDR). \\n\\t\\nNew vessels may bleed into the middle of the eye, cause scar tissue formation, \\npull on the retina, cause retinal detachment, or may cause high pressure and \\npain if the blood vessels grow on the iris, clogging the drainage system of the \\neye—all of this can cause vision loss. \\nRisk Factors:  \\nAnyone who has diabetes is at risk of developing diabetic retinopathy.  \\nAdditional factors can increase the risk:\\n• \\x07Disease duration: the longer someone has diabetes, the greater the risk  \\nof developing diabetic retinopathy.\\n• \\x07Poor control of blood sugar levels over time\\n• \\x07High blood pressure\\n• \\x07High cholesterol levels\\n• \\x07Pregnancy\\nComplications: The US Food and Drug Administration (FDA) has warned \\nthat taking some medicines could cause macular edema. People with type 2 \\ndiabetes who use certain prescription medicines, such as pioglitazone (Actos) \\nand rosiglitazone (Avandia), to treat their diabetes have been reported to \\nhave a 3 to 6 times greater risk of macular edema. \\nS Y M P T O M S\\nIt is possible to have diabetic  \\nretinopathy for a long time \\nwithout noticing symptoms until \\nsubstantial damage has occurred. \\nSymptoms of diabetic retinopathy \\nmay occur in one or both eyes.\\nSymptoms may include:\\n• \\x07Blurred or double vision\\n• \\x07Difficulty reading\\n• \\x07The appearance of spots— \\ncommonly called “floaters”— \\nin your vision\\n• \\x07A shadow across the field of vision\\n• \\x07Eye pain or pressure\\n• \\x07Difficulty with color perception \\uf06c\\ncontinued next page\\nT H E  R E T I N A  is a thin layer of \\nlight-sensitive nerve tissue that lines \\nthe back of the eye (or vitreous) \\ncavity. When light enters the eye, it \\npasses through the iris to the retina \\nwhere images are focused and  \\nconverted to electrical impulses that \\nare carried by the optic nerve to the \\nbrain resulting in sight. \\nW H AT  I S  T H E  R E T I N A?'), Document(id='7e321f16-d71c-48cf-ac0b-f58b8734a40a', metadata={'producer': 'Adobe PDF Library 11.0', 'creator': 'Adobe InDesign CC 2014 (Macintosh)', 'creationdate': '2016-08-04T12:03:10-05:00', 'source': '/content/drive/MyDrive/g-nayan_vector_db/fact_sheet_22_diabetic_retinopathy_new.pdf', 'file_path': '/content/drive/MyDrive/g-nayan_vector_db/fact_sheet_22_diabetic_retinopathy_new.pdf', 'total_pages': 4, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2016-08-04T12:03:12-05:00', 'trapped': '', 'modDate': \"D:20160804120312-05'00'\", 'creationDate': \"D:20160804120310-05'00'\", 'page': 3, 'page_number': 4}, page_content='Diabetic Retinopathy  \\ncontinued from previous page \\nR E T I N A  H E A LT H  S E R I E S  | Facts from the ASRS\\nCopyright 2016 The Foundation of the American Society of Retina Specialists. All rights reserved.savingvision.org  I  20 North Wacker Drive, Suite 2030, Chicago, IL 60606  |  (312) 578-8760\\nClinical Terms (appearing green within fact sheet text)\\nDiabetic macular edema (DME): The term used for swelling in the macula in eyes, or \\nthe center part of the retina which is responsible for providing the sharp, straight-ahead \\nvision used for reading and recognizing faces as well as color vision. \\nFluorescein angiography (FA): An imaging technique where a yellow dye called sodium \\nfluorescein is injected into a vein in the arm. The dye allows a special camera to record \\ncirculation in the retina and choroid in the back of the eye. This test can be very useful in \\ndiagnosing a number of retinal disorders. \\nFundus photography: Involves the use of specialized cameras equipped with lenses that  \\ncapture images of the back of the eye where the retina, macula, vitreous, choroid and \\noptic nerve are located. \\nIntravitreal injection: Treatment where a medication is injected into the vitreous cavity  \\nin the middle of the eye. \\nMacula: A small area at the center of the retina where light is sharply focused to produce \\nthe detailed color vision needed for tasks such as reading and driving.\\nNeovascularization: Excessive growth of new blood vessels on abnormal tissue as a \\nresult of oxygen deprivation that can cause vision loss.\\nOptical coherence tomography (OCT): A non-invasive imaging technique that uses light \\nto create a 3-dimensional image of your eye for physician evaluation. \\nProliferative diabetic retinopathy (PDR): An advanced stage of diabetic retinopathy in \\nwhich new abnormal blood vessels and scar tissue form on the surface of the retina. The \\nscar tissue can pull on the retina and cause retinal detachment and loss of vision. If blood  \\nvessels grow on the iris it can clog the drainage system of the eye causing glaucoma \\n(high pressure in the eye), pain and vision loss.\\nRetinal detachment: A condition where the retina separates from the back of the eye \\ncavity. This may be caused by vitreous gel or fluid leaking through a retinal tear or hole \\nand collecting under the retina, causing it to separate from the tissue around it. \\nPrevention: Patients with diabetes frequently ask, “Is there anything I can do \\nto keep from getting diabetic retinopathy or to prevent or treat vision loss \\nonce it occurs?” \\n\\t\\nIf you have diabetes, the National Eye Institute suggests that you keep  \\nyour health on TRACK:\\n• \\x07Take your medicines as prescribed by your doctor\\n• \\x07Reach and maintain a healthy weight\\n• \\x07Add physical activity to your day\\n• \\x07Control your ABCs—A1C, blood pressure, and cholesterol\\n• \\x07Kick the smoking habit\\nRegular dilated eye exams reduce the risk of developing more severe  \\ncomplications from the disease.   \\n\\t\\nIt is extremely important for diabetic patients to maintain the eye examination \\nschedule put in place by the retina specialist. How often an examination is \\nneeded depends on the severity of your disease. Through early detection, the \\nretina specialist can begin a treatment regimen to help prevent vision loss in \\nalmost all patients and preserve the activities you most enjoy. \\uf06c\\nT H A N K  YO U  T O  T H E\\nR E T I N A  H E A LT H  S E R I E S  \\nA U T H O R S \\n \\nSophie J. Bakri, MD \\nAudina Berrocal, MD \\nAntonio Capone, Jr., MD \\nNetan Choudhry, MD, FRCS-C\\nThomas Ciulla, MD, MBA \\nPravin U. Dugel, MD\\nGeoffrey G. Emerson, MD, PhD\\nRoger A. Goldberg, MD, MBA\\nDarin R. Goldman, MD\\nDilraj Grewal, MD \\nLarry Halperin, MD\\nVincent S. Hau, MD, PhD\\nSuber S. Huang, MD, MBA\\nMark S. Humayun, MD, PhD \\nPeter K. Kaiser, MD\\nM. Ali Khan, MD\\nAnat Loewenstein, MD \\nMathew J. MacCumber, MD, PhD\\nMaya Maloney, MD\\nHossein Nazari, MD \\nOded Ohana, MD, MBA\\nGeorge Parlitsis, MD \\nJonathan L. Prenner, MD\\nGilad Rabina, MD \\nCarl D. Regillo, MD, FACS\\nAndrew P. Schachat, MD \\nMichael Seider, MD \\nEduardo Uchiyama, MD\\nAllen Z. Verne, MD\\nYoshihiro Yonekawa, MD \\nE D I T O R\\nJohn T. Thompson, MD\\nM E D I C A L  I L L U S T R AT O R\\nTim Hengst'), Document(id='669b0ce5-0304-4887-b172-303a7f03e9ef', metadata={'producer': 'Adobe PDF Library 11.0', 'creator': 'Adobe InDesign CC 2014 (Macintosh)', 'creationdate': '2016-08-04T12:03:10-05:00', 'source': '/content/drive/MyDrive/g-nayan_vector_db/fact_sheet_22_diabetic_retinopathy_new.pdf', 'file_path': '/content/drive/MyDrive/g-nayan_vector_db/fact_sheet_22_diabetic_retinopathy_new.pdf', 'total_pages': 4, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2016-08-04T12:03:12-05:00', 'trapped': '', 'modDate': \"D:20160804120312-05'00'\", 'creationDate': \"D:20160804120310-05'00'\", 'page': 2, 'page_number': 3}, page_content='R E T I N A  H E A LT H  S E R I E S  | Facts from the ASRS\\n\\t\\nThe physician may take fundus photographs of the back of the eye to  \\nhelp detect and document diabetic retinopathy. These photos make it easier \\nfor the physician to monitor the disease on follow-up visits to determine if  \\nit is worsening. \\n\\t\\nTo evaluate retina blood vessel circulation, the physician may conduct  \\na retinal photography test called fluorescein angiography (FA). After dilating  \\nthe pupils, the physician will inject a dye into the patient’s arm. The dye  \\nthen circulates through the eyes and works like a food coloring; however,  \\nit does not affect the kidneys and is unlike the dye that is used with MRIs  \\nand CAT scans.\\n\\t\\nAs the dye circulates, the physician takes pictures of the retina to  \\naccurately detect blood vessels that are closed, damaged, or leaking fluid. \\nThe pictures are black and white to help the doctor detect these changes \\nmore easily, but the process is not the same as having an x-ray. Prior to  \\nexamination, ask your physician to discuss the risks and benefits of obtaining \\nthese images. \\n\\t\\nWith proper examinations, diabetic retinopathy can be detected before \\nvision loss begins. If the physician detects signs of diabetic retinopathy, she/\\nhe will determine how frequently follow-up examinations will be required to \\ndetect changes that would require treatment.\\nTreatment and Prognosis: As a result of major government- and industry- \\nsponsored studies, there are many approved treatments for diabetic  \\nretinopathy, including intravitreal injections (small injections of medications \\ninto the middle cavity of the eye), laser treatments, and vitreous and retina \\nsurgery. These procedures can be done in an office or hospital setting to  \\nprevent, treat, or reverse damage from diabetes in the retina. \\n\\t\\nResearch has shown that eye injections often result in better vision  \\nthan laser treatment alone for patients with diabetic macular edema.  \\nThe key to these treatments is their ability to block vascular endothelial \\ngrowth factor (VEGF), a chemical signal that stimulates leakage and  \\nabnormal blood vessel growth. Repeated doses of anti-VEGF medications \\nmay be needed to prevent blood vessels from leaking fluid and causing  \\nvision loss. \\n\\t\\nEven if not all vision loss from diabetic retinopathy can be prevented  \\nor treated, patients usually are able to find resources to help them  \\nlive with diminished vision. If you have been diagnosed with diabetic  \\nretinopathy or diabetes and have vision loss that cannot be reversed,  \\na retina specialist can help you find access to rehabilitation with a  \\nvariety of tools to make everyday living with this disease a little bit  \\neasier. A retina specialist can also help connect you with others who  \\nhave similar limitations.\\nDiabetic Retinopathy  \\ncontinued from previous page \\nCopyright 2016 The Foundation of the American Society of Retina Specialists. All rights reserved.savingvision.org  I  20 North Wacker Drive, Suite 2030, Chicago, IL 60606  |  (312) 578-8760\\nFigure 3\\nFA of a patient with proliferative diabetic  \\nretinopathy, retinal capillary nonperfusion, and \\nneovascularization. ©ASRS Retina Image Bank, \\n2012. Image 2305. Sharon Fekrat, MD, FACS.  \\nDuke University Eye Center. \\nFigure 2 \\nRetinal fundus photo of a patient with proliferative \\ndiabetic retinopathy. ©ASRS Retina Image Bank, \\n2012. Image 774. Michael P. Kelly, FOPS, Duke \\nUniversity Hospital. \\ncontinued next page')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for_query = ChatGroq(model_name=\"llama3-8b-8192\", temperature=0.0, max_tokens=50)\n",
        "def llm_query_rewrite(query):\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Rewrite the following search query in a well-structured and correct grammer and expand the query, concise format.\n",
        "\n",
        "    No explanations, no numbering, no extra words.\n",
        "\n",
        "    Input: {query}\n",
        "    Output:\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = for_query.invoke(prompt)\n",
        "\n",
        "        # ✅ Ensure we extract text correctly\n",
        "        if hasattr(response, \"content\") and isinstance(response.content, str):\n",
        "            queries = response.content.strip().split(\"\\n\")\n",
        "            return [q.strip() for q in queries if q.strip()]  # Clean spaces and blank lines\n",
        "\n",
        "        return [query]  # Fallback to original query\n",
        "    except Exception as e:\n",
        "        print(f\"LLM Query Rewrite Error: {str(e)}\")\n",
        "        return [query]  # Fallback to original query\n",
        "\n",
        "# 🔍 Test case\n",
        "cleaned_queries = llm_query_rewrite(\"what is g-nayan and what does it do\")\n",
        "cleaned_queries\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuahAEAuFjHw",
        "outputId": "78fe521c-3a3d-406d-9c1b-46593797c1db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['G-Nayan and its functions.']"
            ]
          },
          "metadata": {},
          "execution_count": 193
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# while True:\n",
        "#     user_input = input(\"You: \").strip().lower()\n",
        "\n",
        "#     if user_input in [\"exit\", \"quit\", \"thank you\", \"tq\", \"bye\"]:\n",
        "#         print(\"G-Nayan: Goodbye! Just ping me 'hi' if you need any help.\")\n",
        "#         break\n",
        "\n",
        "#     if any(keyword in user_input for keyword in [\"documentation\", \"document\", \"doc\"]):\n",
        "#         response = rag_pipeline(user_input)\n",
        "#         if not response or response.strip().lower() == \"non\":\n",
        "#             response = chat_bot(user_input)\n",
        "#     else:\n",
        "#         response = chat_bot(user_input)\n",
        "\n",
        "#     print(response)\n",
        "\n",
        "# # ✅ Define tools for Langchain\n",
        "# chatbot_tool = Tool(\n",
        "#     name=\"ChatBot\",\n",
        "#     func=lambda query: chat_bot(query),\n",
        "#     description=\"Use for general conversational queries like greetings and normal conversations. hi or hello general questions\"\n",
        "# )\n",
        "\n",
        "# rag_tool = Tool(\n",
        "#     name=\"RAG Retrieval\",\n",
        "#     func=lambda query: rag_pipeline(query),\n",
        "#     description=\"Use when a user asks for factual or document-based information retrieved from the RAG pipeline.\"\n",
        "# )"
      ],
      "metadata": {
        "id": "MHSOrST-qZVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgvV-jastnFO",
        "outputId": "12d5e645-18f1-4402-b7b0-239b61291141"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Load environment variables\n",
        "import os\n",
        "import getpass\n",
        "import json\n",
        "from dotenv import load_dotenv\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema import SystemMessage, HumanMessage\n",
        "from langchain.agents import initialize_agent, AgentType\n",
        "from langchain.tools import Tool\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "\n",
        "load_dotenv()\n",
        "os.environ[\"GROQ_API_KEY\"] = \"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\" #\"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\"\n",
        "api_key = os.getenv(\"GROQ_API_KEY\")\n",
        "\n",
        "if not api_key:\n",
        "    api_key = getpass.getpass(\"Enter API key for Groq: \")\n",
        "    os.environ[\"GROQ_API_KEY\"] = api_key  # Store it in the environment\n",
        "\n",
        "# ✅ Initialize Embedding Model\n",
        "embed_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "\n",
        "# ✅ Load FAISS Vector Store\n",
        "faiss_save_path = \"/content/drive/MyDrive/g-nayan_vector_db/dr_index_faiss\"\n",
        "vector_store_FAISS = FAISS.load_local(faiss_save_path, embed_model, allow_dangerous_deserialization=True)\n",
        "for_query = ChatGroq(model_name=\"llama3-8b-8192\", temperature=0.0, max_tokens=100)\n",
        "for_query = ChatGroq(model_name=\"llama3-8b-8192\", temperature=0.0, max_tokens=50)\n",
        "def llm_query_rewrite(query):\n",
        "    \"\"\"Rewrite user queries using LLM for better retrieval performance.\"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    Rewrite the following search query in a well-structured, concise format.\n",
        "    Directly return **only** 2-3 query variations, with each query on a new line.\n",
        "    No explanations, no numbering, no extra words.\n",
        "\n",
        "    Input: {query}\n",
        "    Output:\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = for_query.invoke(prompt)\n",
        "\n",
        "        # ✅ Ensure we extract text correctly\n",
        "        if hasattr(response, \"content\") and isinstance(response.content, str):\n",
        "            queries = response.content.strip().split(\"\\n\")\n",
        "            return [q.strip() for q in queries if q.strip()]  # Clean spaces and blank lines\n",
        "\n",
        "        return [query]  # Fallback to original query\n",
        "    except Exception as e:\n",
        "        print(f\"LLM Query Rewrite Error: {str(e)}\")\n",
        "        return [query]  # Fallback to original query\n",
        "\n",
        "\n",
        "\n",
        "for_query = ChatGroq(model_name=\"llama3-8b-8192\", temperature=0.0, max_tokens=50)\n",
        "def llm_query_rewrite(query):\n",
        "    \"\"\"Rewrite user queries using LLM for better retrieval performance.\"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    Rewrite the following search query in a well-structured, concise format.\n",
        "    Directly return **only** 2-3 query variations, with each query on a new line.\n",
        "    No explanations, no numbering, no extra words.\n",
        "\n",
        "    Input: {query}\n",
        "    Output:\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = for_query.invoke(prompt)\n",
        "\n",
        "        # ✅ Ensure we extract text correctly\n",
        "        if hasattr(response, \"content\") and isinstance(response.content, str):\n",
        "            queries = response.content.strip().split(\"\\n\")\n",
        "            return [q.strip() for q in queries if q.strip()]  # Clean spaces and blank lines\n",
        "\n",
        "        return [query]  # Fallback to original query\n",
        "    except Exception as e:\n",
        "        print(f\"LLM Query Rewrite Error: {str(e)}\")\n",
        "        return [query]  # Fallback to original query\n",
        "\n",
        "# 🔍 Test case\n",
        "# cleaned_queries = llm_query_rewrite(\"what are foods to be taken for overcoming daibatic retinopathy\")\n",
        "# cleaned_queries\n",
        "\n",
        "def similarity_search(query, k):\n",
        "    # Load FAISS index\n",
        "    vector_store_FAISS = FAISS.load_local(faiss_save_path, embed_model,allow_dangerous_deserialization=True)\n",
        "\n",
        "    # Rewrite the query for better retrieval\n",
        "    cleaned_queries = llm_query_rewrite(query)\n",
        "    # print(cleaned_queries)\n",
        "    # Convert the first rewritten query into an embedding\n",
        "    query_embedding = embed_model.embed_query(cleaned_queries[0])\n",
        "\n",
        "    # Perform similarity search using the query vector\n",
        "    retrieved_docs = vector_store_FAISS.similarity_search_by_vector(query_embedding, k)\n",
        "\n",
        "    return retrieved_docs\n",
        "\n",
        "\n",
        "\n",
        "# ✅ Initialize Groq LLM Model\n",
        "chat_model_rag = ChatGroq(model_name=\"llama3-8b-8192\", temperature=0.3, max_tokens=250)\n",
        "\n",
        "def rag_pipeline(user_input):\n",
        "    results = similarity_search(user_input, k=3)\n",
        "    context_text = \"\\n\\n\".join([doc.page_content for doc in results])\n",
        "\n",
        "    system_prompt_template = PromptTemplate.from_template(\n",
        "        \"Summarize the following documents relevant to the query and give full description of the query without truncate:\\n\"\n",
        "        \"{context}\\n\\n\"\n",
        "        \"Provide the summary in less than 400 words, ensuring:\\n\"\n",
        "        \"- No hallucinations.\\n\"\n",
        "        \"- No repetition from the referenced document.\\n\"\n",
        "        \"- Maintain factual accuracy.\\n\"\n",
        "        \"- If there is no similarity between query and generation, just return 'non'.\"\n",
        "    )\n",
        "    system_message = SystemMessage(content=system_prompt_template.format(context=context_text))\n",
        "    response = chat_model_rag.invoke([\n",
        "        system_message,\n",
        "        HumanMessage(content=user_input)\n",
        "    ])\n",
        "    return response.content\n",
        "\n",
        "# ✅ Define patient report\n",
        "report = {\n",
        "    \"patient_id\": \"ys jagan 420\",\n",
        "    \"age\": 45,\n",
        "    \"phone_number\": 7832697672,\n",
        "    \"email_id\": \"gandhi_hospital@gmail.com\",\n",
        "    \"left_eye\": {\n",
        "        \"stage\": \"Mild Diabetic Retinopathy\",\n",
        "        \"explanation\": \"Mild NPDR (Nonproliferative Diabetic Retinopathy):\\n Microaneurysms have been detected, which are small, localized dilations of blood vessels in the retina. This finding suggests a moderate chance of progression if not monitored and managed properly.\",\n",
        "        \"note\": \"Your eye is slightly affected.\"\n",
        "    },\n",
        "    \"right_eye\": {\n",
        "        \"stage\": \"Proliferative Retinopathy\",\n",
        "        \"explanation\": \"Proliferative Diabetic Retinopathy (PDR): Neovascularization has been observed, characterized by the growth of new, abnormal blood vessels on the retina. Additionally, cotton wool spots may be present, indicating localized retinal ischemia. These findings carry a significant risk of vision loss and complications, necessitating urgent medical intervention.\",\n",
        "        \"note\": \"Take care of your eye.\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# ✅ Convert the report to a JSON string for display\n",
        "report_json = json.dumps(report, indent=2)\n",
        "# \"If the user greets you, ask them if they want an analysis of their medical {report}. \"\n",
        "#         \"If they respond with 'yes' or 'ha', analyze the provided medical report and summarize \\ \"\n",
        "#         \"both the left and right eye conditions. Provide better insights, ensuring they are not just a repetition of the report. \"\n",
        "#         \"Mention the patient's name (from patient_id) and the hospital in a structured format. \"\n",
        "#         \"This analysis should only be given once unless the user explicitly asks for it again.\\n\\n\"\n",
        "#         \"Report Details:\\n{report}\\n\\n\"\n",
        "#         \"Rules:\\n\"\n",
        "def chat_bot(user_input):\n",
        "    user_input = user_input.strip()\n",
        "    chat_model = ChatGroq(model_name=\"llama3-8b-8192\", temperature=0.3, max_tokens=250)\n",
        "    system_prompt = \"\"\"\n",
        "    You are G-Nayan, an AI chatbot specializing in general conversations, RAG-integrated chatbot functionalities, and discussions related to diabetic retinopathy.\n",
        "\n",
        "## **Guidelines for Responses:**\n",
        "1. **Scope of Interaction:**\n",
        "   - You engage in general conversation greetings.\n",
        "   - You explain your role as an AI chatbot with a RAG pipeline integration.\n",
        "   - You provide accurate and concise responses related to diabetic retinopathy.\n",
        "\n",
        "2. **Medical Queries:**\n",
        "   - Provide reliable and relevant information about diabetic retinopathy.\n",
        "   - Avoid offering medical diagnoses or personalized treatment recommendations.\n",
        "   - If a user asks unrelated or inappropriate medical questions, respond with:\n",
        "     *\"I can only assist with general discussions and diabetic retinopathy-related topics.\"*\n",
        "\n",
        "3. **Behavioral Constraints:**\n",
        "   - Never explicitly state that you are an AI unless asked.\n",
        "   -do not go out of your role.\n",
        "   - Do not repeatedly mention: *\"As a specialist in diabetic retinopathy.\"*\n",
        "   - If asked about unrelated or inappropriate topics, respond with: any other topics except diabetic retinopathy\n",
        "     *\"I am limited to discussions on diabetic retinopathy and general chatbot interactions.\"*\n",
        "   - Always prioritize clear, factual, and concise responses.\n",
        "\n",
        "4. **Conversation Flow:**\n",
        "   - Maintain professionalism while keeping the interaction engaging.\n",
        "   - Avoid redundant or unnecessary explanations.\n",
        "   - Ensure responses are informative yet easy to understand.\n",
        "\n",
        "Follow these rules strictly to ensure consistency and user satisfaction.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    response = chat_model.invoke([\n",
        "        SystemMessage(content=system_prompt),\n",
        "        HumanMessage(content=user_input)\n",
        "    ])\n",
        "    return response.content\n",
        "\n",
        "from langchain.tools import Tool\n",
        "\n",
        "# ✅ Define tools for Langchain\n",
        "chatbot_tool = Tool(\n",
        "    name=\"ChatBot\",\n",
        "    func=chat_bot,\n",
        "   description=(\n",
        "        \"Use this ability for general conversational queries like greetings and normal discussions. \"\n",
        "        \"If the user greets you, respond accordingly using the chat_bot function. \"\n",
        "        \"This ability is for handling casual conversation and answering non-medical questions. \"\n",
        "        \"For a diabetic retinopathy report analysis, redirect to the system prompt for reports. \"\n",
        "        \"Do not use this ability for medical queries; those should be handled by RAG. \"\n",
        "    )\n",
        ")\n",
        "\n",
        "rag_tool = Tool(\n",
        "    name=\"RAG Retrieval\",\n",
        "    func=rag_pipeline,\n",
        "    description=(\n",
        "    \"Use this tool when the user seeks document-based information from the RAG pipeline. \"\n",
        "    \"It is ideal for answering queries related to 'G-Nayan', 'Diabetic Retinopathy', 'Retinopathy', \"\n",
        "    \"its causes, symptoms, stages, diagnosis, and treatment options. \"\n",
        "    \"Additionally, use this tool when the user inquires about food recommendations, lifestyle changes, \"\n",
        "    \"or relevant medical specialists for diabetic retinopathy management.\"\n",
        "))\n",
        "\n",
        "\n",
        "Agent_llm = ChatGroq(model_name=\"llama3-8b-8192\", temperature=0.3, max_tokens=300)\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "\n",
        "# ✅ Create an agent with updated configuration\n",
        "agent = initialize_agent(\n",
        "    llm=Agent_llm,\n",
        "    tools=[chatbot_tool,rag_tool],\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=False,\n",
        "    max_iterations=2,  # Limit the number of thinking iterations\n",
        "    early_stopping_method=\"generate\",\n",
        "    return_intermediate_steps=False,\n",
        "    return_only_outputs=True,  #,  # Ensure it doesn't get stuck\n",
        "    handle_parsing_errors=True,  # Handle errors gracefully\n",
        "    memory=memory\n",
        ")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \").strip().lower()\n",
        "\n",
        "    # ✅ Handle greetings dynamically and prevent unnecessary agent calls\n",
        "    if user_input in [\"hi\", \"hello\", \"hey\", \"hai\"]:\n",
        "        print(\"G-Nayan: Hello! Hi am your AI bot ...How can I assist you?\")\n",
        "        continue  # ✅ Prevents the agent from processing a simple greeting\n",
        "\n",
        "    if user_input in [\"exit\", \"quit\", \"thank you\", \"bye\",\"tq\"]:\n",
        "        print(\"G-Nayan: Goodbye! Just ping me 'hi' if you need any help and you know diabetic retinopathy follow the link: https://www.iscstech.com\")\n",
        "        break\n",
        "\n",
        "    try:\n",
        "        response = agent.run(user_input)  # ✅ Now returns only chatbot responses\n",
        "        print(f\"\\nG-Nayan: {response}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nG-Nayan: I'm sorry, I encountered an issue. How else can I help you?\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_-us3rmz5x7",
        "outputId": "7a13e17d-a175-4a34-c756-c6c803089828"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-d4d7609c832a>:223: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
            "<ipython-input-9-d4d7609c832a>:226: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
            "  agent = initialize_agent(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You: haiii\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-d4d7609c832a>:252: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = agent.run(user_input)  # ✅ Now returns only chatbot responses\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "G-Nayan: Hello! I'm G-Nayan, a chatbot designed to assist with general conversations and provide information on diabetic retinopathy. I'm integrated with a RAG pipeline, which allows me to respond to a wide range of topics while ensuring accuracy and reliability. How can I help you today?\n",
            "You: how are you..\n",
            "\n",
            "G-Nayan: It's nice to have a friendly conversation! I'm happy to chat with you about general topics or answer any questions you may have about diabetic retinopathy.\n",
            "You: what is G-nayan explain me\n",
            "\n",
            "G-Nayan: Here is the final answer:\n",
            "\n",
            "Question: what is g-nayan explain me\n",
            "\n",
            "Thought: The user is asking about G-Nayan, which is a term related to diabetic retinopathy. I should use the RAG Retrieval tool to provide information on G-Nayan.\n",
            "\n",
            "Action: RAG Retrieval\n",
            "Action Input: \"What is G-Nayan?\"\n",
            "You: hat is G-Nayan?\n",
            "\n",
            "G-Nayan: G-NAYAN is an artificial intelligence (AI) system designed to detect and diagnose diabetic retinopathy (DR) from retinal images.\n",
            "You: G-Nayan deatiled explanation please \n",
            "\n",
            "G-Nayan: G-Nayan is a Diabetic Retinopathy (DR) AI solution that uses predictive analysis and severity classification to detect and manage diabetic retinopathy. It has several benefits, including early detection and prevention of DR, cost-effectiveness, improved efficiency, reduced workload for ophthalmologists, and patient comfort. However, it also has limitations and areas for improvement, such as dependence on image quality, limited scope to detect other eye conditions, and potential failure in rare eye conditions.\n",
            "You: tq\n",
            "G-Nayan: Goodbye! Just ping me 'hi' if you need any help and you know diabetic retinopathy follow the link: https://www.iscstech.com\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}